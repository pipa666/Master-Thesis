
@misc{noauthor_computational_nodate,
	title = {Computational {Thinking} {Education} in {K}–12},
	url = {https://mitpress.mit.edu/9780262543477/computational-thinking-education-in-k12/},
	abstract = {A guide to computational thinking education, with a focus on artificial intelligence literacy and the integration of computing and physical objects.Computing...},
	language = {en-US},
	urldate = {2023-09-20},
	journal = {MIT Press},
}

@article{lodi_computational_2021,
	title = {Computational {Thinking}, {Between} {Papert} and {Wing}},
	volume = {30},
	issn = {1573-1901},
	url = {https://doi.org/10.1007/s11191-021-00202-5},
	doi = {10.1007/s11191-021-00202-5},
	abstract = {The pervasiveness of Computer Science (CS) in today’s digital society and the extensive use of computational methods in other sciences call for its introduction in the school curriculum. Hence, Computer Science Education is becoming more and more relevant. In CS K-12 education, computational thinking (CT) is one of the abused buzzwords: different stakeholders (media, educators, politicians) give it different meanings, some more oriented to CS, others more linked to its interdisciplinary value. The expression was introduced by two leading researchers, Jeannette Wing (in 2006) and Seymour Papert (much early, in 1980), each of them stressing different aspects of a common theme. This paper will use a historical approach to review, discuss, and put in context these first two educational and epistemological approaches to CT. We will relate them to today’s context and evaluate what aspects are still relevant for CS K-12 education. Of the two, particular interest is devoted to “Papert’s CT,” which is the lesser-known and the lesser-studied. We will conclude that “Wing’s CT” and “Papert’s CT,” when correctly understood, are both relevant to today’s computer science education. From Wing, we should retain computer science’s centrality, CT being the (scientific and cultural) substratum of the technical competencies. Under this interpretation, CT is a lens and a set of categories for understanding the algorithmic fabric of today’s world. From Papert, we should retain the constructionist idea that only a social and affective involvement of students into the technical content will make programming an interdisciplinary tool for learning (also) other disciplines. We will also discuss the often quoted (and often unverified) claim that CT automatically “transfers” to other broad 21st century skills. Our analysis will be relevant for educators and scholars to recognize and avoid misconceptions and build on the two core roots of CT.},
	language = {en},
	number = {4},
	urldate = {2023-09-08},
	journal = {Science \& Education},
	author = {Lodi, Michael and Martini, Simone},
	month = aug,
	year = {2021},
	pages = {883--908},
}

@article{reiss_use_2021,
	title = {The use of {AI} in education: {Practicalities} and ethical considerations},
	volume = {19},
	issn = {1474-8479},
	shorttitle = {The use of {AI} in education},
	url = {https://scienceopen.com/hosted-document?doi=10.14324/LRE.19.1.05},
	doi = {10.14324/LRE.19.1.05},
	abstract = {There is a wide diversity of views on the potential for artificial intelligence (AI), ranging from overenthusiastic pronouncements about how it is imminently going to transform our lives to alarmist predictions about how it is going to cause everything from mass unemployment to the destruction of life as we know it. In this article, I look at the practicalities of AI in education and at the attendant ethical issues it raises. My key conclusion is that AI in the near- to medium-term future has the potential to enrich student learning and complement the work of (human) teachers without dispensing with them. In addition, AI should increasingly enable such traditional divides as ‘school versus home’ to be straddled with regard to learning. AI offers the hope of increasing personalization in education, but it is accompanied by risks of learning becoming less social. There is much that we can learn from previous introductions of new technologies in school to help maximize the likelihood that AI can help students both to flourish and to learn powerful knowledge. Looking further ahead, AI has the potential to be transformative in education, and it may be that such benefits will first be seen for students with special educational needs. This is to be welcomed.},
	language = {en},
	number = {1},
	urldate = {2023-02-21},
	journal = {London Review of Education},
	author = {Reiss, Michael J.},
	month = feb,
	year = {2021},
	keywords = {notion},
}

@misc{nakano_webgpt_2022,
	title = {{WebGPT}: {Browser}-assisted question-answering with human feedback},
	shorttitle = {{WebGPT}},
	url = {http://arxiv.org/abs/2112.09332},
	doi = {10.48550/arXiv.2112.09332},
	abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
	month = jun,
	year = {2022},
	note = {arXiv:2112.09332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{oppenlaender_taxonomy_2022,
	title = {A {Taxonomy} of {Prompt} {Modifiers} for {Text}-{To}-{Image} {Generation}},
	url = {http://arxiv.org/abs/2204.13988},
	doi = {10.48550/arXiv.2204.13988},
	abstract = {Text-to-image generation has seen an explosion of interest since 2021. Today, beautiful and intriguing digital images and artworks can be synthesized from textual inputs ("prompts") with deep generative models. Online communities around text-to-image generation and AI generated art have quickly emerged. This paper identifies six types of prompt modifiers used by practitioners in the online community based on a 3-month ethnographic study. The novel taxonomy of prompt modifiers provides researchers a conceptual starting point for investigating the practice of text-to-image generation, but may also help practitioners of AI generated art improve their images. We further outline how prompt modifiers are applied in the practice of "prompt engineering." We discuss research opportunities of this novel creative practice in the field of Human-Computer Interaction (HCI). The paper concludes with a discussion of broader implications of prompt engineering from the perspective of Human-AI Interaction (HAI) in future applications beyond the use case of text-to-image generation and AI generated art.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Oppenlaender, Jonas},
	month = jul,
	year = {2022},
	note = {arXiv:2204.13988 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Multimedia, H.5, H.m, J.5},
}

@article{williamson_historical_2020,
	title = {Historical threads, missing links, and future directions in {AI} in education},
	volume = {45},
	issn = {1743-9884},
	url = {https://doi.org/10.1080/17439884.2020.1798995},
	doi = {10.1080/17439884.2020.1798995},
	number = {3},
	urldate = {2023-09-07},
	journal = {Learning, Media and Technology},
	author = {Williamson, Ben and Eynon, Rebecca},
	month = jul,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/17439884.2020.1798995},
	pages = {223--235},
}

@article{zawacki-richter_systematic_2019,
	title = {Systematic review of research on artificial intelligence applications in higher education – where are the educators?},
	volume = {16},
	issn = {2365-9440},
	url = {https://doi.org/10.1186/s41239-019-0171-0},
	doi = {10.1186/s41239-019-0171-0},
	abstract = {According to various international reports, Artificial Intelligence in Education (AIEd) is one of the currently emerging fields in educational technology. Whilst it has been around for about 30 years, it is still unclear for educators how to make pedagogical advantage of it on a broader scale, and how it can actually impact meaningfully on teaching and learning in higher education. This paper seeks to provide an overview of research on AI applications in higher education through a systematic review. Out of 2656 initially identified publications for the period between 2007 and 2018, 146 articles were included for final synthesis, according to explicit inclusion and exclusion criteria. The descriptive results show that most of the disciplines involved in AIEd papers come from Computer Science and STEM, and that quantitative methods were the most frequently used in empirical studies. The synthesis of results presents four areas of AIEd applications in academic support services, and institutional and administrative services: 1. profiling and prediction, 2. assessment and evaluation, 3. adaptive systems and personalisation, and 4. intelligent tutoring systems. The conclusions reflect on the almost lack of critical reflection of challenges and risks of AIEd, the weak connection to theoretical pedagogical perspectives, and the need for further exploration of ethical and educational approaches in the application of AIEd in higher education.},
	number = {1},
	urldate = {2023-09-07},
	journal = {International Journal of Educational Technology in Higher Education},
	author = {Zawacki-Richter, Olaf and Marín, Victoria I. and Bond, Melissa and Gouverneur, Franziska},
	month = oct,
	year = {2019},
	keywords = {Artificial intelligence, Higher education, Intelligent tutoring systems, Machine learning, Systematic review},
	pages = {39},
}

@article{chen_application_2020,
	title = {Application and theory gaps during the rise of {Artificial} {Intelligence} in {Education}},
	volume = {1},
	issn = {2666920X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X20300023},
	doi = {10.1016/j.caeai.2020.100002},
	abstract = {Considering the increasing importance of Artiﬁcial Intelligence in Education (AIEd) and the absence of a comprehensive review on it, this research aims to conduct a comprehensive and systematic review of inﬂuential AIEd studies. We analyzed 45 articles in terms of annual distribution, leading journals, institutions, countries/ regions, the most frequently used terms, as well as theories and technologies adopted. We also evaluated deﬁnitions of AIEd from broad and narrow perspectives and clariﬁed the relationship among AIEd, Educational Data Mining, Computer-Based Education, and Learning Analytics. Results indicated that: 1) there was a continuingly increasing interest in and impact of AIEd research; 2) little work had been conducted to bring deep learning technologies into educational contexts; 3) traditional AI technologies, such as natural language processing were commonly adopted in educational contexts, while more advanced techniques were rarely adopted, 4) there was a lack of studies that both employ AI technologies and engage deeply with educational theories. Findings suggested scholars to 1) seek the potential of applying AI in physical classroom settings; 2) spare efforts to recognize detailed entailment relationships between learners’ answers and the desired conceptual understanding within intelligent tutoring systems; 3) pay more attention to the adoption of advanced deep learning algorithms such as generative adversarial network and deep neural network; 4) seek the potential of NLP in promoting precision or personalized education; 5) combine biomedical detection and imaging technologies such as electroencephalogram, and target at issues regarding learners’ during the learning process; and 6) closely incorporate the application of AI technologies with educational theories.},
	language = {en},
	urldate = {2023-09-07},
	journal = {Computers and Education: Artificial Intelligence},
	author = {Chen, Xieling and Xie, Haoran and Zou, Di and Hwang, Gwo-Jen},
	year = {2020},
	pages = {100002},
}

@article{sullivan_notes_2000,
	title = {Notes from a {Marine} biologist's daughter: {On} the art and science of attention},
	volume = {70},
	copyright = {Copyright Harvard Educational Review Summer 2000},
	issn = {00178055},
	shorttitle = {Notes from a {Marine} biologist's daughter},
	url = {https://www.proquest.com/docview/212261265/abstract/4D28BC35589F42F3PQ/1},
	abstract = {Using an autobiographical lens, McCrary Sullivan explores the sensory and emotional aspects of attending and their implications for teaching, learning and research. Her poetry and prose attempt to awaken in the reader an artistic engagement with the various means of attention.},
	language = {English},
	number = {2},
	urldate = {2023-08-29},
	journal = {Harvard Educational Review},
	author = {Sullivan, Anne McCrary},
	year = {2000},
	note = {Num Pages: 18
Place: Cambridge, United States
Publisher: Harvard Educational Review},
	keywords = {Art, Attention, Attention deficit hyperactivity disorder, Education, Learning, Poetry, Prose, Reading, Teaching},
	pages = {211--227},
}

@misc{noauthor_creative_nodate,
	title = {Creative {Methodologies} - {Richardson}-{L}.-{Writing}-a-method-of-{Inquiry}.pdf - {All} {Documents}},
	url = {https://helsinkifi.sharepoint.com/sites/CreativeMethodologies/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FCreativeMethodologies%2FShared%20Documents%2FReadings%2Fwriting%2FRichardson%2DL%2E%2DWriting%2Da%2Dmethod%2Dof%2DInquiry%2Epdf&parent=%2Fsites%2FCreativeMethodologies%2FShared%20Documents%2FReadings%2Fwriting&p=true&ga=1},
	urldate = {2023-08-28},
}

@article{sundar_rise_2020,
	title = {Rise of {Machine} {Agency}: {A} {Framework} for {Studying} the {Psychology} of {Human}–{AI} {Interaction} ({HAII})},
	volume = {25},
	issn = {1083-6101},
	shorttitle = {Rise of {Machine} {Agency}},
	url = {https://doi.org/10.1093/jcmc/zmz026},
	doi = {10.1093/jcmc/zmz026},
	abstract = {Advances in personalization algorithms and other applications of machine learning have vastly enhanced the ease and convenience of our media and communication experiences, but they have also raised significant concerns about privacy, transparency of technologies and human control over their operations. Going forth, reconciling such tensions between machine agency and human agency will be important in the era of artificial intelligence (AI), as machines get more agentic and media experiences become increasingly determined by algorithms. Theory and research should be geared toward a deeper understanding of the human experience of algorithms in general and the psychology of Human–AI interaction (HAII) in particular. This article proposes some directions by applying the dual-process framework of the Theory of Interactive Media Effects (TIME) for studying the symbolic and enabling effects of the affordances of AI-driven media on user perceptions and experiences.},
	number = {1},
	urldate = {2023-08-28},
	journal = {Journal of Computer-Mediated Communication},
	author = {Sundar, S Shyam},
	month = mar,
	year = {2020},
	pages = {74--88},
}

@misc{xu_transitioning_2023,
	title = {Transitioning to human interaction with {AI} systems: {New} challenges and opportunities for {HCI} professionals to enable human-centered {AI}},
	shorttitle = {Transitioning to human interaction with {AI} systems},
	url = {http://arxiv.org/abs/2105.05424},
	doi = {10.48550/arXiv.2105.05424},
	abstract = {While AI has benefited humans, it may also harm humans if not appropriately developed. The focus of HCI work is transiting from conventional human interaction with non-AI computing systems to interaction with AI systems. We conducted a high-level literature review and a holistic analysis of current work in developing AI systems from an HCI perspective. Our review and analysis highlight the new changes introduced by AI technology and the new challenges that HCI professionals face when applying the human-centered AI (HCAI) approach in the development of AI systems. We also identified seven main issues in human interaction with AI systems, which HCI professionals did not encounter when developing non-AI computing systems. To further enable the implementation of the HCAI approach, we identified new HCI opportunities tied to specific HCAI-driven design goals to guide HCI professionals in addressing these new issues. Finally, our assessment of current HCI methods shows the limitations of these methods in support of developing AI systems. We propose alternative methods that can help overcome these limitations and effectively help HCI professionals apply the HCAI approach to the development of AI systems. We also offer strategic recommendations for HCI professionals to effectively influence the development of AI systems with the HCAI approach, eventually developing HCAI systems.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Xu, Wei and Dainoff, Marvin J. and Ge, Liezhong and Gao, Zaifeng},
	month = mar,
	year = {2023},
	note = {arXiv:2105.05424 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@article{wen_sense_2022,
	title = {The sense of agency in perception, behaviour and human–machine interactions},
	volume = {1},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-022-00030-6},
	doi = {10.1038/s44159-022-00030-6},
	abstract = {The sense of agency refers to the subjective feeling of controlling one’s own actions, and through them, external events. The sense of agency is a byproduct of human movements and also greatly shapes perception and behaviour. Furthermore, research on human–machine interaction has highlighted the importance of the sense of agency in joint control between humans and automated systems. In this Review, we first provide an overview of how the sense of agency influences human perception and how the perceptual effects of the sense of agency are used to measure this subjective feeling. Second, we review how the sense of agency modulates behaviour, including action selection, goal-directed actions, and social cognition. Third, we introduce theoretical and neural accounts of how the sense of agency arises. Finally, we explain how the sense of agency applies to human–machine interactions, an area that is rapidly developing and increasingly linked to daily life.},
	language = {en},
	number = {4},
	urldate = {2023-08-28},
	journal = {Nature Reviews Psychology},
	author = {Wen, Wen and Imamizu, Hiroshi},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Agency, Human behaviour, Perception, Psychology},
	pages = {211--222},
}

@book{smith_practice-led_2009,
	title = {Practice-led {Research}, {Research}-led {Practice} in the {Creative} {Arts}},
	isbn = {978-0-7486-3630-3},
	url = {https://www.degruyter.com/document/doi/10.1515/9780748636303/html},
	language = {en},
	urldate = {2023-08-24},
	publisher = {Edinburgh University Press},
	author = {Smith, Hazel and Dean, Roger},
	month = jun,
	year = {2009},
	doi = {10.1515/9780748636303},
}

@article{kukul_computational_2019,
	title = {Computational {Thinking} {Self}-{Efficacy} {Scale}: {Development}, {Validity} and {Reliability}},
	volume = {18},
	url = {http://dx.doi.org/10.15388/INFEDU.2019.07},
	doi = {10.15388/infedu.2019.07},
	abstract = {The aim of this study is to develop a self-efficacy measuring tool that can predict the computational thinking skill that is seen as one of the 21st century’s skills. According to literature review, an item pool was established and expert opinion was consulted for the created item pool. The study group of this study consists of 319 students educated at the level of secondary school. As a result of the exploratory factor analysis, the scale consisted of 18 items under four factors. The factors are Reasoning, Abstraction, Decomposition and Generalization. As a result of applied reliability analysis, the Cronbach Alpha reliability coefficient can be seen to be calculated as .884 for the whole self-efficacy scale consisting of 18 items. Confirmative factor analysis results and fit indexes were checked, and fit indexes of the scale were seen to have good and acceptable fits. Based on these findings, the Computational Thinking Self-efficacy Scale is a valid and reliable tool that may be used in measuring to predict Computational Thinking.},
	number = {1},
	journal = {Informatics in Education},
	author = {KUKUL, Volkan and KARATAS, Serçin},
	month = apr,
	year = {2019},
	note = {Publisher: Vilnius University Press},
	pages = {151--164},
}

@article{ertugrul-akyol_development_2019,
	title = {Development of {Computational} {Thinking} {Scale}: {Validity} and {Reliability} {Study}},
	volume = {5},
	url = {http://dx.doi.org/10.12973/ijem.5.3.421},
	doi = {10.12973/ijem.5.3.421},
	abstract = {Computational thinking is a way of thinking that covers 21st century skills and includes new generation concepts such as robotics, coding, informatics and information construction. Computational thinking has reached an important point especially in the field of science in line with the rapid developments in technology. Robotics applications, software-based activities, STEM (Science, Technology, Engineering, Math) education and problem-based studies are some of the areas where this thinking is used. In this study, which is based on this point, it is aimed to develop a scale for computational thinking. Exploratory sequential design, one of the mixed research methods, was used in the study. First of all, a detailed literature review was conducted and needs analysis was carried out. This study consists of two stages. In the first stage, exploratory factor analysis was performed and analyzed with SPSS 23 program. In the second stage, confirmatory factor analysis was performed and analyzed with LISREL 9.2 program. As a result of the study, the goodness of fit indexes of the scale was found. According to this; X2/df value 1.81; NNFI value 0.97; NFI value 0.93; CFI value 0.98; RMR value 0.05; SRMR value 0.04; AGFI value 0.91 and GFI value was found to be 0.93. When the reliability values of the study were examined, Cronbach’s Alpha value was found to be 0.86. As a result of the research, a computational thinking scale consisting of 3 factors and 30 items was developed. This scale was developed for prospective teachers and can be used at all levels of prospective teachers.},
	number = {3},
	journal = {International Journal of Educational Methodology},
	author = {Ertugrul-Akyol, Buket},
	month = aug,
	year = {2019},
	note = {Publisher: Eurasian Society of Educational Research},
	pages = {421--432},
}

@misc{IleniaEnvisioning,
	title = {Envisioning a {Computational} {Thinking} {Assessment} {Tool}},
	abstract = {Recent work on Computational Thinking (CT) has focused on proposing new curricula but in many cases the assessment phase has been overlooked. The issue is critical because appropriate assessment is needed to facilitate the incorporation of CT in the curriculum. What is now clear from the existing literature is that there is a need to build on top of the existing multiple forms of assessments, in order to integrate multiple approaches and reach a comprehensive assessment of CT learning. In this paper, we envision a system that integrates different types of assessments while providing an intuitive interface in order to allow teachers to see and supervise the overview of the learning process, with the possibility to assess individually the student’s learning. To assess the suitability of our idea, we describe the Proof of Concept of a mobile application to assist CT assessment, and we discuss the challenges that need to be solved to create such an application.},
	author = {{Ilenia Fronza} and {C. Pahl}},
}

@article{Greene2015Measuring,
	title = {Measuring {Cognitive} {Engagement} {With} {Self}-{Report} {Scales}: {Reflections} {From} {Over} 20 {Years} of {Research}},
	volume = {50},
	url = {http://dx.doi.org/10.1080/00461520.2014.989230},
	doi = {10.1080/00461520.2014.989230},
	abstract = {Research spanning 20 years is reviewed as it relates to the measurement of cognitive engagement using self-report scales. The author's research program is at the forefront of the review, although the review is couched within the broader context of the research on motivation and cognitive engagement that began in the early 1990s. The theoretical origins of self-report instruments are examined, along with the early measurement findings and struggles. Research in science, technology, engineering, and mathematics contexts are highlighted. The author concludes that self-report data have made significant and important contributions to the understanding of motivation and cognitive engagement. However, the evidence also suggests a need to develop and use multiple approaches to measuring engagement in academic work rather than rely only on self-report instruments. Some alternatives to self-report measures are suggested here and throughout this issue.},
	number = {1},
	journal = {Educational Psychologist},
	author = {Greene, Barbara A.},
	month = jan,
	year = {2015},
	note = {Publisher: Informa UK Limited},
	pages = {14--30},
}

@article{Cansu2019Overview,
	title = {An {Overview} of {Computational} {Thinking}},
	volume = {3},
	url = {http://dx.doi.org/10.21585/IJCSES.V3I1.53},
	doi = {10.21585/ijcses.v3i1.53},
	abstract = {Computers and smart devices have become ubiquitous staples of our lives. Computers and computer-controlled devices are used in all industries from medicine to engineering, and textile production. One field where computers have inevitably spread into is education, and one pre-requisite of controlling computers, or increasing the level and efficiency of our control over them, is making human-computer interaction as efficient as possible. This process of efficient and effective computer use, known as “Computer-like Thinking” or “Computational Thinking”, is seen as a field with the potential to support individual and societal development in our rapidly progressing world and to provide significant economic benefits. The fundamental concepts and scope of this field have been delineated in diverse manners by different researchers. Similarly, researchers have also advanced distinct critical viewpoints towards and potential benefits of computational thinking. This study aims to first define the concept of computational thinking by referencing source literature, then analyze the aims of certain criticisms of the field, and discuss the fundamental elements of computational thinking and contemporary research on these elements.},
	number = {1},
	journal = {International Journal of Computer Science Education in Schools},
	author = {Cansu, Fatih Kursat and Cansu, Sibel Kilicarslan},
	month = apr,
	year = {2019},
	note = {Publisher: ICT in Practice},
	pages = {17--30},
}

@article{Allsop2019Assessing,
	title = {Assessing computational thinking process using a multiple evaluation approach},
	volume = {19},
	url = {http://dx.doi.org/10.1016/J.IJCCI.2018.10.004},
	doi = {10.1016/j.ijcci.2018.10.004},
	abstract = {Abstract This study explored the ways that the Computational Thinking (CT) process can be evaluated in a classroom environment. Thirty Children aged 10–11 years, from a primary school in London took part in a game-making project using the Scratch and Alice 2.4 applications for eight months. For the focus of this specific paper, data from participant observations, informal conversations, problem-solving sheets, semi-structured interviews and children’s completed games were used to make sense of elements of the computational thinking process and approaches to evaluate these elements in a computer game design context. The discussions around what CT consists, highlighted the complex structure of computational thinking and the interaction between the elements of artificial intelligence (AI), computer, cognitive, learning and psychological sciences. This also emphasised the role of metacognition in the Computational Thinking process. These arguments illustrated that it is not possible to evaluate Computational Thinking using only programming constructs, as CT process provides opportunities for developing many other skills and concepts. Therefore a multiple evaluation approach should be adopted to illustrate the full learning scope of the Computational Thinking Process. Using the support of literature review and the findings of the data analysis I proposed a multiple approach evaluation model where ‘computational concepts’, ‘metacognitive practices’, and ‘learning behaviours’ were discussed as the main elements of the CT process. Additionally, in order to investigate these dimensions within a game-making context, computer game design was also included in this evaluation model.},
	journal = {International Journal of Child-Computer Interaction},
	author = {Allsop, Yasemin},
	month = mar,
	year = {2019},
	note = {Publisher: Elsevier BV},
	pages = {30--55},
}

@misc{LComputational,
	title = {Computational {Thinking} {Frameworks} used in {Computational} {Thinking} {Assessment} in {Higher} {Education}. {A} {Systematized} {Literature} {Review}},
	abstract = {We propose Computational Thinking (CT) as an innovative pedagogical approach with broad application. Research and current industry trends illustrate that students should have a solid computational thinking ability in order to have the skills required for future jobs in Artificial Intelligence. Due to current social issues regarding COVID-19 and natural disasters, we are rapidly moving towards a cyberspace era where many citizens will conduct their work online. Understanding the foundations and tools of computation - e.g., ion, decomposition, pattern recognition - is critical for any student to be prepared for the digital AI age. Believing students should be fully prepared for future jobs that involve computation, we developed a CT module on a Learning Management System (LMS). We have collected data of students who took our CT course module. We looked into the students' activity records and analyzed the number of students' views on the pages and the number of participants on each quiz. We counted the total number of engagements of the ten components in the CT course module. Ultimately, we believe that our modules had a greater impact on those students who were newer to computational thinking, over those who had prior experience and were enrolled in upper-level computational courses.  American Society for Engineering Education, 2021},
	author = {{L. M. C. Castro} and {H. Shoaib} and {K. Douglas} and {G. Dogan} and {Y. Song} and {D. Surek}},
}

@article{furnham_self-estimates_2001,
	title = {Self-estimates of intelligence: culture and gender difference in self and other estimates of both general (g) and multiple intelligences},
	volume = {31},
	issn = {01918869},
	shorttitle = {Self-estimates of intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191886900002324},
	doi = {10.1016/S0191-8869(00)00232-4},
	language = {en},
	number = {8},
	urldate = {2023-06-05},
	journal = {Personality and Individual Differences},
	author = {Furnham, A},
	month = dec,
	year = {2001},
	pages = {1381--1405},
}

@article{korkmaz_validity_2012,
	title = {A validity and reliability study of the {Online} {Cooperative} {Learning} {Attitude} {Scale} ({OCLAS})},
	volume = {59},
	issn = {0360-1315},
	url = {https://www.sciencedirect.com/science/article/pii/S036013151200142X},
	doi = {10.1016/j.compedu.2012.05.021},
	abstract = {Determination of students' attitudes towards online cooperative learning is an important issue, which has not been studied adequately. In the literature, there are few scales to measure the attitude towards online cooperative learning for which validity and reliability have been proven. The main purpose of this study is to develop an attitude scale in order to specify students' attitudes towards online cooperative learning. The sample group is composed of 599 students for the first application and 242 students for the second. In order to detect the validity of the scale, exploratory and confirmatory factor analyses, item factor total correlations, corrected correlations and item discriminations were conducted. In order to assess the reliability of the scale, the level of internal consistency and the stability levels were calculated. OCLAS is a five-point Likert-type scale and includes 17 items that can be gathered under 2 factors. The analyses provided evidence that the Online Cooperative Learning Attitude Scale (OCLAS) is a valid and reliable scale that can be used in order to determine students' attitudes towards cooperative learning in online environments.},
	language = {en},
	number = {4},
	urldate = {2023-05-30},
	journal = {Computers \& Education},
	author = {Korkmaz, Özgen},
	month = dec,
	year = {2012},
	keywords = {Cooperative/collaborative learning, Distance education and telelearning, Media in education, Teaching/learning strategies},
	pages = {1162--1169},
}

@article{tsai_computational_2021,
	title = {The {Computational} {Thinking} {Scale} for {Computer} {Literacy} {Education}},
	volume = {59},
	issn = {0735-6331, 1541-4140},
	url = {http://journals.sagepub.com/doi/10.1177/0735633120972356},
	doi = {10.1177/0735633120972356},
	abstract = {Computational thinking has received tremendous attention from computer science educators and educational researchers in the last decade. However, most prior literature defines computational thinking as thinking outcomes rather than thinking processes. Based on Selby and Woodland’s framework, this study developed and validated the Computational Thinking Scale (CTS) to assess all students’ thought processes of computational thinking for both general and specific problem-solving contexts in five dimensions: abstraction, decomposition, algorithmic thinking, evaluation and generalization. A survey including 25 candidate items for CTS as well as demographic variables was administered to 388 junior high school students in Taiwan. An explorative factor analysis using the principal axis method with the oblimin rotation was used to validate the scale. Finally, 19 items were extracted successfully under the designed five dimensions, with a total explained variance of 64.03\% and an overall reliability of 0.91. Results of the demographic comparisons showed that boys had a greater disposition than girls in decomposition thinking when solving problems using computer programming. In addition, programming learning experience, especially self-directed learning and after-school learning, had significant positive effects on all dimensions of CTS. Several future studies are suggested using this tool.},
	language = {en},
	number = {4},
	urldate = {2023-05-30},
	journal = {Journal of Educational Computing Research},
	author = {Tsai, Meng-Jung and Liang, Jyh-Chong and Hsu, Chung-Yuan},
	month = jul,
	year = {2021},
	pages = {579--602},
}

@article{korkmaz_validity_2017,
	title = {A validity and reliability study of the computational thinking scales ({CTS})},
	volume = {72},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563217300055},
	doi = {10.1016/j.chb.2017.01.005},
	abstract = {It is possible to define Computational Thinking briefly as having the knowledge, skill and attitudes necessary to be able to use the computers in the solution of the life problems for production purposes. In this study, a scale has been developed for the purpose of determining the levels of computational thinking skills (CTS) of the students. CTS is a five-point likert type scale and consists of 29 items that could be collected under five factors. The study group of this work consists of 726 students educated at the levels of associate degree and undergraduate degree with formal education in Amasya University for the first application. For the second application 580 students who were educated in pedagogical formation education via distance education in Amasya University. The validity and reliability of the scale have been studied by conducting exploratory factor analysis, confirmatory factor analysis, item distinctiveness analyses, internal consistency coefficients and constancy analyses. As a result of the conducted analyses, it has been concluded that the scale is a valid and reliable measurement tool that could measure the computational thinking skills of the students. In addition; the digital age individuals are expected to have the computational thinking skill, and at what degree they have these skills, the revelation of whether the levels they have are sufficient or not are a requirement. Within this frame, it could be said that the scale could make significant contributions to the literature.},
	language = {en},
	urldate = {2023-05-29},
	journal = {Computers in Human Behavior},
	author = {Korkmaz, Özgen and Çakir, Recep and Özden, M. Yaşar},
	month = jul,
	year = {2017},
	keywords = {Computer-mediated communication, Pedagogical issues, Programming and programming languages, Teaching/learning strategies, Valuation methodologies},
	pages = {558--569},
}

@article{klein_ai_2023,
	chapter = {Opinion},
	title = {{AI} machines aren’t ‘hallucinating’. {But} their makers are},
	issn = {0261-3077},
	url = {https://www.theguardian.com/commentisfree/2023/may/08/ai-machines-hallucinating-naomi-klein},
	abstract = {Tech CEOs want us to believe that generative AI will benefit humanity. They are kidding themselves},
	language = {en-GB},
	urldate = {2023-05-15},
	journal = {The Guardian},
	author = {Klein, Naomi},
	month = may,
	year = {2023},
	keywords = {Artificial intelligence (AI), Technology, US news},
}

@article{falkenberg_innovation_2022,
	title = {Innovation in {Technology} {Instead} of {Thinking}? {Assetization} and {Its} {Epistemic} {Consequences} in {Academia}},
	issn = {0162-2439},
	shorttitle = {Innovation in {Technology} {Instead} of {Thinking}?},
	url = {https://doi.org/10.1177/01622439221140003},
	doi = {10.1177/01622439221140003},
	abstract = {This paper draws on the notion of the asset to better understand the role of innovative research technologies in researchers’ practices and decisions. Faced with both the need to accumulate academic capital to make a living in academia and with many uncertainties about the future, researchers must find ways to anticipate future academic revenues. We illustrate that innovative research technologies provide a suitable means for doing so: First, because they promise productivity through generating interesting data and hence publications. Second, because they allow a signaling of innovativeness in contexts where research is evaluated, even across disciplinary boundaries. As such, enrolling innovative research technologies as assets allows researchers to bridge partly conflicting valuations of productivity and innovativeness they are confronted with. However, the employment of innovative technologies in anticipation of future academic revenues is not always aligned with what researchers value epistemically. Nevertheless, considerations about potential future academic revenues derived from innovative research technologies sometimes seem to override particular epistemic valuations. Illustrating these dynamics, we show that processes of assetization in academia can have significant epistemic consequences which are important to unpack.},
	language = {en},
	urldate = {2023-05-13},
	journal = {Science, Technology, \& Human Values},
	author = {Falkenberg, Ruth and Fochler, Maximilian},
	month = dec,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {01622439221140003},
}

@article{ahmad_artificial_2020,
	title = {Artificial intelligence in education: a panoramic review},
	shorttitle = {Artificial intelligence in education},
	journal = {DOI: https://doi. org/10.35542/osf. io/zvu2n},
	author = {Ahmad, Kashif and Qadir, J. and Al-Fuqaha, A. and Iqbal, W. and El-Hassan, A. and Benhaddou, D. and Ayyash, M.},
	year = {2020},
	keywords = {notion},
}

@misc{noauthor_we_nodate,
	title = {Do we need international rules for artificial intelligence? {Three} points in the {EU} proposal for {AI} regulation that you should know {\textbar} {University} of {Helsinki}},
	shorttitle = {Do we need international rules for artificial intelligence?},
	url = {https://www.helsinki.fi/en/news/artificial-intelligence/do-we-need-international-rules-artificial-intelligence-three-points-eu-proposal-ai-regulation-you-should-know},
	abstract = {There is no dedicated legislation on AI in Finland or the European Union. The European Commission has recently made a proposal for regulation of AI. Susanna Lindroos-Hovinheimo, professor in public law at the University of Helsinki, will tell us what we need to know about the proposal – and why it is important.},
	language = {en},
	urldate = {2023-04-26},
}

@article{wing_computational_2006,
	title = {Computational thinking},
	volume = {49},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1118178.1118215},
	doi = {10.1145/1118178.1118215},
	abstract = {It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.},
	number = {3},
	urldate = {2023-04-18},
	journal = {Communications of the ACM},
	author = {Wing, Jeannette M.},
	month = mar,
	year = {2006},
	keywords = {notion},
	pages = {33--35},
}

@article{holmes_state_2022,
	title = {State of the art and practice in {AI} in education},
	volume = {57},
	issn = {1465-3435},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejed.12533},
	doi = {10.1111/ejed.12533},
	abstract = {Recent developments in Artificial Intelligence (AI) have generated great expectations for the future impact of AI in education and learning (AIED). Often these expectations have been based on misunderstanding current technical possibilities, lack of knowledge about state-of-the-art AI in education, and exceedingly narrow views on the functions of education in society. In this article, we provide a review of existing AI systems in education and their pedagogic and educational assumptions. We develop a typology of AIED systems and describe different ways of using AI in education and learning, show how these are grounded in different interpretations of what AI and education is or could be, and discuss some potential roadblocks on the AIED highway.},
	language = {en},
	number = {4},
	urldate = {2023-04-17},
	journal = {European Journal of Education},
	author = {Holmes, Wayne and Tuomi, Ilkka},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejed.12533},
	keywords = {notion},
	pages = {542--570},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	abstract = {Artiﬁcial intelligence (AI) researchers have been developing and reﬁning large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and diﬃcult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4’s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artiﬁcial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reﬂections on societal inﬂuences of the recent technological leap and future research directions.},
	language = {en},
	urldate = {2023-03-31},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
}

@article{koro-ljungberg_dt_nodate,
	title = {D...a…t…a…, {Data}++, {Data}, and {Some} {Problematics}},
	language = {en},
	author = {Koro-Ljungberg, Mirka and MacLure, Maggie and Ulmer, Jasmine},
	keywords = {notion},
}

@inproceedings{cain_gpteammate_2023,
	title = {{GPTeammate}: {A} {Design} {Fiction} on the {Use} of {Variants} of the {GPT} {Language} {Model} as {Cognitive} {Partners} for {Active} {Learning} in {Higher} {Education}},
	isbn = {978-1-939797-68-1},
	shorttitle = {{GPTeammate}},
	url = {https://www.learntechlib.org/primary/p/221996/},
	abstract = {This paper explores the potential for variants of the GPT language model, such as ChatGPT and GPT-3, to participate as cognitive partners in active learning scenarios with higher education students. Using the ‘design-fiction’ approach to speculate on future scenarios in human-computer interaction (HCI), this paper explores the potential for a hypothetical AI application- “GPTeammate” - to participate as fully realized collaborative social entities in active learning processes and projects in graduate-level learning, design, and technology (LDT) courses. In exploring this future design, I...},
	language = {en},
	urldate = {2023-04-18},
	publisher = {Association for the Advancement of Computing in Education (AACE)},
	author = {Cain, William},
	month = mar,
	year = {2023},
	keywords = {notion},
	pages = {1293--1298},
}

@article{hamalainen_evaluating_2023,
	title = {Evaluating {Large} {Language} {Models} in {Generating} {Synthetic} {HCI} {Research} {Data}: a {Case} {Study}},
	abstract = {Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any fndings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.},
	language = {en},
	author = {Hämäläinen, Perttu},
	year = {2023},
}

@article{brennan_new_2012,
	title = {New frameworks for studying and assessing the development of computational thinking},
	abstract = {Computational thinking is a phrase that has received considerable attention over the past several years – but there is little agreement about what computational thinking encompasses, and even less agreement about strategies for assessing the development of computational thinking in young people. We are interested in the ways that design-based learning activities – in particular, programming interactive media – support the development of computational thinking in young people. Over the past several years, we have developed a computational thinking framework that emerged from our studies of the activities of interactive media designers. Our context is Scratch – a programming environment that enables young people to create their own interactive stories, games, and simulations, and then share those creations in an online community with other young programmers from around the world.},
	language = {en},
	author = {Brennan, Karen and Resnick, Mitchel},
	year = {2012},
	keywords = {notion},
}

@article{doleck_algorithmic_2017,
	title = {Algorithmic thinking, cooperativity, creativity, critical thinking, and problem solving: exploring the relationship between computational thinking skills and academic performance},
	volume = {4},
	issn = {2197-9995},
	shorttitle = {Algorithmic thinking, cooperativity, creativity, critical thinking, and problem solving},
	url = {https://doi.org/10.1007/s40692-017-0090-9},
	doi = {10.1007/s40692-017-0090-9},
	abstract = {The continued call for twenty-first century skills renders computational thinking a topical subject of study, as it is increasingly recognized as a fundamental competency for the contemporary world. Yet its relationship to academic performance is poorly understood. In this paper, we explore the association between computational thinking and academic performance. We test a structural model—employing a partial least squares approach—to assess the relationship between computational thinking skills and academic performance. Surprisingly, we find no association between computational thinking skills and academic performance (except for a link between cooperativity and academic performance). These results are discussed respecting curricular mandated instruction in higher-order thinking skills and the importance of curricular alignment between instructional objectives and evaluation approaches for successfully teaching and learning twenty-first-century skills.},
	language = {en},
	number = {4},
	urldate = {2023-04-17},
	journal = {Journal of Computers in Education},
	author = {Doleck, Tenzin and Bazelais, Paul and Lemay, David John and Saxena, Anoop and Basnet, Ram B.},
	month = dec,
	year = {2017},
	keywords = {Academic performance, CEGEP students, Computational thinking, Computational thinking skills, Curricular alignment, notion},
	pages = {355--369},
}

@article{xu_transitioning_2023-1,
	title = {Transitioning to {Human} {Interaction} with {AI} {Systems}: {New} {Challenges} and {Opportunities} for {HCI} {Professionals} to {Enable} {Human}-{Centered} {AI}},
	volume = {39},
	issn = {1044-7318},
	shorttitle = {Transitioning to {Human} {Interaction} with {AI} {Systems}},
	url = {https://doi.org/10.1080/10447318.2022.2041900},
	doi = {10.1080/10447318.2022.2041900},
	abstract = {While AI has benefited humans, it may also harm humans if not appropriately developed. The priority of current HCI work should focus on transiting from conventional human interaction with non-AI computing systems to interaction with AI systems. We conducted a high-level literature review and a holistic analysis of current work in developing AI systems from an HCI perspective. Our review and analysis highlight the new changes introduced by AI technology and the new challenges that HCI professionals face when applying the human-centered AI (HCAI) approach in the development of AI systems. We also identified seven main issues in human interaction with AI systems, which HCI professionals did not encounter when developing non-AI computing systems. To further enable the implementation of the HCAI approach, we identified new HCI opportunities tied to specific HCAI-driven design goals to guide HCI professionals addressing these new issues. Finally, our assessment of current HCI methods shows the limitations of these methods in support of developing HCAI systems. We propose the alternative methods that can help overcome these limitations and effectively help HCI professionals apply the HCAI approach to the development of AI systems. We also offer strategic recommendation for HCI professionals to effectively influence the development of AI systems with the HCAI approach, eventually developing HCAI systems.},
	number = {3},
	urldate = {2023-03-30},
	journal = {International Journal of Human–Computer Interaction},
	author = {Xu, Wei and Dainoff, Marvin J. and Ge, Liezhong and Gao, Zaifeng},
	month = feb,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2022.2041900},
	keywords = {notion},
	pages = {494--518},
}

@article{rusanen_action_2021,
	title = {Action control, forward models and expected rewards: representations in reinforcement learning},
	volume = {199},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Action control, forward models and expected rewards},
	url = {https://link.springer.com/10.1007/s11229-021-03408-w},
	doi = {10.1007/s11229-021-03408-w},
	abstract = {The fundamental cognitive problem for active organisms is to decide what to do next in a changing environment. In this article, we analyze motor and action control in computational models that utilize reinforcement learning (RL) algorithms. In reinforcement learning, action control is governed by an action selection policy that maximizes the expected future reward in light of a predictive world model. In this paper we argue that RL provides a way to explicate the so-called action-oriented views of cognitive systems in representational terms.},
	language = {en},
	number = {5-6},
	urldate = {2023-04-18},
	journal = {Synthese},
	author = {Rusanen, Anna-Mari and Lappi, Otto and Kuokkanen, Jesse and Pekkanen, Jami},
	month = dec,
	year = {2021},
	keywords = {notion},
	pages = {14017--14033},
}

@incollection{penstein_rose_vygotsky_2018,
	address = {Cham},
	title = {Vygotsky {Meets} {Backpropagation}: {Artificial} {Neural} {Models} and the {Development} of {Higher} {Forms} of {Thought}},
	volume = {10947},
	isbn = {978-3-319-93842-4 978-3-319-93843-1},
	shorttitle = {Vygotsky {Meets} {Backpropagation}},
	url = {http://link.springer.com/10.1007/978-3-319-93843-1_42},
	abstract = {In this paper we revisit Vygotsky’s developmental model of concept formation, and use it to discuss learning in artificial neural networks. We study learning in neural networks from a learning science point of view, asking whether it is possible to construct systems that have developmental patterns that align with empirical studies on concept formation. We put the state-of-the-art Inception-v3 image recognition architecture in an experimental setting that highlights differences and similarities in algorithmic and human cognitive processes.},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Tuomi, Ilkka},
	editor = {Penstein Rosé, Carolyn and Martínez-Maldonado, Roberto and Hoppe, H. Ulrich and Luckin, Rose and Mavrikis, Manolis and Porayska-Pomsta, Kaska and McLaren, Bruce and du Boulay, Benedict},
	year = {2018},
	doi = {10.1007/978-3-319-93843-1_42},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {570--583},
}

@article{heer_agency_2019,
	title = {Agency plus automation: {Designing} artificial intelligence into interactive systems},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Agency plus automation},
	url = {https://pnas.org/doi/full/10.1073/pnas.1807184115},
	doi = {10.1073/pnas.1807184115},
	abstract = {Much contemporary rhetoric regards the prospects and pitfalls of using artificial intelligence techniques to automate an increasing range of tasks, especially those once considered the purview of people alone. These accounts are often wildly optimistic, understating outstanding challenges while turning a blind eye to the human labor that undergirds and sustains ostensibly “automated” services. This long-standing focus on purely automated methods unnecessarily cedes a promising design space: one in which computational assistance augments and enriches, rather than replaces, people’s intellectual work. This tension between human agency and machine automation poses vital challenges for design and engineering. In this work, we consider the design of systems that enable rich, adaptive interaction between people and algorithms. We seek to balance the often-complementary strengths and weaknesses of each, while promoting human control and skillful action. We share case studies of interactive systems we have developed in three arenas—data wrangling, exploratory analysis, and natural language translation—that integrate proactive computational support into interactive systems. To improve outcomes and support learning by both people and machines, we describe the use of shared representations of tasks augmented with predictive models of human capabilities and actions. We conclude with a discussion of future prospects and scientific frontiers for intelligence augmentation research.},
	language = {en},
	number = {6},
	urldate = {2023-03-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heer, Jeffrey},
	month = feb,
	year = {2019},
	keywords = {notion},
	pages = {1844--1850},
}

@misc{qadir_engineering_2022,
	title = {Engineering {Education} in the {Era} of {ChatGPT}: {Promise} and {Pitfalls} of {Generative} {AI} for {Education}},
	shorttitle = {Engineering {Education} in the {Era} of {ChatGPT}},
	url = {https://www.techrxiv.org/articles/preprint/Engineering_Education_in_the_Era_of_ChatGPT_Promise_and_Pitfalls_of_Generative_AI_for_Education/21789434/1},
	doi = {10.36227/techrxiv.21789434.v1},
	abstract = {Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {TechRxiv},
	author = {Qadir, Junaid},
	month = dec,
	year = {2022},
	keywords = {notion},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning, notion},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample eﬃciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@article{dale_gpt-3_2021,
	title = {{GPT}-3: {What}’s it good for?},
	volume = {27},
	issn = {1351-3249, 1469-8110},
	shorttitle = {{GPT}-3},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/gpt3-whats-it-good-for/0E05CFE68A7AC8BF794C8ECBE28AA990},
	doi = {10.1017/S1351324920000601},
	abstract = {GPT-3 made the mainstream media headlines this year, generating far more interest than we’d normally expect of a technical advance in NLP. People are fascinated by its ability to produce apparently novel text that reads as if it was written by a human. But what kind of practical applications can we expect to see, and can they be trusted?},
	language = {en},
	number = {1},
	urldate = {2023-03-04},
	journal = {Natural Language Engineering},
	author = {Dale, Robert},
	month = jan,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	keywords = {notion},
	pages = {113--118},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-02-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {notion},
}

@incollection{el_morr_21st_2022,
	address = {Boca Raton},
	edition = {1},
	title = {On the 21st {Century} {Digital} {Toys}: {The} {Paradox} of {Data} {Literacy}},
	isbn = {978-1-00-326124-7},
	shorttitle = {On the 21st {Century} {Digital} {Toys}},
	url = {https://www.taylorfrancis.com/books/9781003261247/chapters/10.1201/9781003261247-21},
	abstract = {In AI and algorithmic-driven societies, well-designed visualizations provide epistemic resources for decision-making. At the same time, they also introduce epistemic risks by hiding design and algorithmic processes of data behind the visual display. This creates a paradox of data literacy. We know how to use visual interfaces, but we are not able to “read” them. In addition to “know-how”, users should also be able to “know-why”. We argue that for the success of AI-driven decisionmaking it becomes crucial to conceive data and visualization literacy in epistemic, inferential and evidential terms.},
	language = {en},
	urldate = {2023-02-21},
	booktitle = {{AI} and {Society}},
	publisher = {Chapman and Hall/CRC},
	author = {Rusanen, Anna-Mari and Vesikukka, Teppo},
	collaborator = {El Morr, Christo},
	month = oct,
	year = {2022},
	doi = {10.1201/9781003261247-21},
	keywords = {notion},
	pages = {283--295},
}

@book{niemi_ai_2023,
	address = {Cham},
	title = {{AI} in {Learning}: {Designing} the {Future}},
	isbn = {978-3-031-09686-0 978-3-031-09687-7},
	shorttitle = {{AI} in {Learning}},
	url = {https://link.springer.com/10.1007/978-3-031-09687-7},
	language = {en},
	urldate = {2023-02-21},
	publisher = {Springer International Publishing},
	editor = {Niemi, Hannele and Pea, Roy D. and Lu, Yu},
	year = {2023},
	doi = {10.1007/978-3-031-09687-7},
	keywords = {notion},
}

@inproceedings{tedre_ct_2021,
	address = {Joensuu Finland},
	title = {{CT} 2.0},
	isbn = {978-1-4503-8488-9},
	url = {https://dl.acm.org/doi/10.1145/3488042.3488053},
	doi = {10.1145/3488042.3488053},
	abstract = {CT has been the central rallying point for K-12 computing education at least since the early 2010s. Many teachers, school administrators, and policymakers have joined the movement. A consensus has emerged over the conceptual landscape of CT.},
	language = {en},
	urldate = {2023-03-27},
	booktitle = {21st {Koli} {Calling} {International} {Conference} on {Computing} {Education} {Research}},
	publisher = {ACM},
	author = {Tedre, Matti and Denning, Peter and Toivonen, Tapani},
	month = nov,
	year = {2021},
	keywords = {notion},
	pages = {1--8},
}

@article{kiyici_meta-analytic_2022,
	title = {A {Meta}-{Analytic} {Reliability} {Generalization} {Study} of the {Computational} {Thinking} {Scale}},
	volume = {13},
	url = {http://dx.doi.org/10.15354/sief.22.ma011},
	doi = {10.15354/sief.22.ma011},
	abstract = {This study aims to analyze the reliability generalization of the computational thinking scale. There are five dimensions of computational thinking: creativity, algorithmic thinking, coopera-tivity, critical thinking, and problem-solving. A Bonett transformation was used to standardize the reliability coefficient of Cronbach’s alpha. A random-effects meta-analysis was conducted since the heterogeneity among the studies was high. Results supported the RG of the computational thinking scale and its sub-dimensions, which were calculated as 0.843 for general, 0.799 for creativity, 0.848 for algorithmic thinking, 0.863 for cooperativity, 0.799 for critical thinking, and 0.817 for problem-solving. Besides that, the moderator analysis was conducted for the sample type, test length, country, and language of the study. According to the findings, there were no significant moderator effects on the reliability estimation.},
	number = {2},
	journal = {Science Insights Education Frontiers},
	author = {Kiyici, G{\textbackslash}" ulbin and Kahraman, Nurcan},
	month = dec,
	year = {2022},
	note = {Publisher: Bonoi Science Advancement and Education LLC},
	pages = {1859--1874},
}

@article{yildirim_developing_2022,
	title = {Developing {Computational} {Thinking} {Scale} for {Primary} {School} {Students} and {Examining} {Students}' {Thinking} {Levels} {According} to {Different} {Variables}},
	url = {http://dx.doi.org/10.53850/joltida.1176173},
	doi = {10.53850/joltida.1176173},
	abstract = {In this study, it was aimed to develop the computational thinking scale for primary school students and to examine the computational thinking levels of primary school students according to different variables (grade level, daily computer use time). The study was carried out in accordance with the general survey model, which is one of the descriptive research types. The study group of the research consisted of students studying in the 1st, 2nd, 3rd and 4th grades of primary schools in Ankara G{\textbackslash}" olbaşı district in the second term of the 2021-2022 academic year. The data were analyzed with the SPSS 21 package program. As a result of the study, a one-dimensional Computational Thinking Scale, whose validity and reliability was tested, was developed. The scale consists of 17 items and is in 3-point Likert type. The internal consistency coefficient of the scale was found to be Cronbach Alpha .92. The developed scale was applied to primary school students in the next stage. As a result of the study, it was found that the computational thinking levels of primary school students differed significantly according to the grade level. On the other hand, it was observed that the students' computational thinking levels differed significantly according to the time spent in front of the computer daily, and the mean of the students' computational thinking scale increased as the daily computer use time increased.},
	journal = {Journal of Learning and Teaching in Digital Age},
	author = {YILDIRIM, Erg{\textbackslash}" un and ULUYOL, Çelebi},
	month = oct,
	year = {2022},
	note = {Publisher: Journal of Learning and Teaching in Digital Age},
}

@inproceedings{de_jong_developing_2022,
	title = {Developing a {Self}-efficacy {Scale} for {Computational} {Thinking} ({CT}-{SES})},
	url = {http://dx.doi.org/10.1145/3564721.3565954},
	doi = {10.1145/3564721.3565954},
	abstract = {Self-efficacy is an important construct in education, as it can influence (among other aspects) perseverance, engagement and success on educational tasks. As such, a student’s Computational Thinking (CT) self-efficacy can have an important influence on, and may be a predictor for, the development and use of CT skills. This poster abstract provides the details of an in-progress study in which we develop a scale to measure CT self-efficacy in different contexts.},
	booktitle = {Koli {Calling} '22: 22nd {Koli} {Calling} {International} {Conference} on {Computing} {Education} {Research}},
	publisher = {ACM},
	author = {De Jong, Imke and Jeuring, Johan},
	month = nov,
	year = {2022},
}

@misc{baillie_beyond_2022,
	title = {Beyond the symbolic vs non-symbolic {AI} debate},
	url = {https://medium.com/@jcbaillie/beyond-the-symbolic-vs-non-symbolic-ai-debate-96dffce7270c},
	abstract = {How can we improve and build beyond the current deep learning paradigm to move towards human-level AI?},
	language = {en},
	urldate = {2023-04-04},
	journal = {Medium},
	author = {Baillie, J. C.},
	month = mar,
	year = {2022},
}

@misc{marcus_deep_2022,
	title = {Deep {Learning} {Is} {Hitting} a {Wall}},
	url = {https://nautil.us/deep-learning-is-hitting-a-wall-238440/},
	abstract = {What would it take for artificial intelligence to make real progress?},
	language = {en-US},
	urldate = {2023-04-04},
	journal = {Nautilus},
	author = {Marcus, Gary},
	month = mar,
	year = {2022},
}

@incollection{pihlajarinne_ai-generated_2019,
	title = {{AI}-generated content: authorship and inventorship in the age of artificial intelligence},
	isbn = {978-1-78811-990-0},
	shorttitle = {{AI}-generated content},
	url = {https://www.elgaronline.com/view/edcoll/9781788119894/9781788119894.00015.xml},
	language = {en},
	urldate = {2023-04-01},
	booktitle = {Online {Distribution} of {Content} in the {EU}},
	publisher = {Edward Elgar Publishing},
	author = {Ballardini, Rosa Maria and Kan, He and Teemu, Roos},
	collaborator = {Pihlajarinne, Taina and Vesala, Juha and Honkkila, Olli},
	year = {2019},
	doi = {10.4337/9781788119900.00015},
	pages = {117--135},
}

@misc{korhonen_invention_2022,
	title = {Invention {Pedagogy} – {The} {Finnish} {Approach} to {Maker} {Education}},
	url = {https://www.routledge.com/Invention-Pedagogy-The-Finnish-Approach-to-Maker-Education/Korhonen-Kangas-Salo/p/book/9781032251974},
	abstract = {This collection, edited and written by the leading scholars and experts of innovation and maker education in Finland, introduces invention pedagogy, a research-based Finnish approach for teaching and learning through multidisciplinary, creative design and making processes in formal school settings.
The book outlines the background of, and need for, invention pedagogy, providing various perspectives for designing and orchestrating the invention process while discussing what can be learned and how},
	language = {en},
	urldate = {2023-03-27},
	journal = {Routledge \& CRC Press},
	author = {Korhonen, Tiina and Kangas, Kaiju and Laura, Salo},
	year = {2022},
}

@article{tamim_what_2011,
	title = {What {Forty} {Years} of {Research} {Says} {About} the {Impact} of {Technology} on {Learning}: {A} {Second}-{Order} {Meta}-{Analysis} and {Validation} {Study}},
	volume = {81},
	issn = {0034-6543, 1935-1046},
	shorttitle = {What {Forty} {Years} of {Research} {Says} {About} the {Impact} of {Technology} on {Learning}},
	url = {http://journals.sagepub.com/doi/10.3102/0034654310393361},
	doi = {10.3102/0034654310393361},
	abstract = {This research study employs a second-order meta-analysis procedure to summarize 40 years of research activity addressing the question, does computer technology use affect student achievement in formal face-to-face classrooms as compared to classrooms that do not use technology? A study-level meta-analytic validation was also conducted for purposes of comparison. An extensive literature search and a systematic review process resulted in the inclusion of 25 meta-analyses with minimal overlap in primary literature, encompassing 1,055 primary studies. The random effects mean effect size of 0.35 was significantly different from zero. The distribution was heterogeneous under the fixed effects model. To validate the second-order meta-analysis, 574 individual independent effect sizes were extracted from 13 out of the 25 meta-analyses. The mean effect size was 0.33 under the random effects model, and the distribution was heterogeneous. Insights about the state of the field, implications for technology use, and prospects for future research are discussed.},
	language = {en},
	number = {1},
	urldate = {2023-03-30},
	journal = {Review of Educational Research},
	author = {Tamim, Rana M. and Bernard, Robert M. and Borokhovski, Eugene and Abrami, Philip C. and Schmid, Richard F.},
	month = mar,
	year = {2011},
	pages = {4--28},
}

@incollection{niemi_introduction_2023,
	address = {Cham},
	title = {Introduction to {AI} in {Learning}: {Designing} the {Future}},
	isbn = {978-3-031-09687-7},
	shorttitle = {Introduction to {AI} in {Learning}},
	url = {https://doi.org/10.1007/978-3-031-09687-7_1},
	abstract = {The introduction chapter focuses on the main questions of the whole book AI in Learning: Designing the Future: (1) How is learning changing when human learning and machine learning are connected and what consequences does this conjunction have for education, also for working life as lifelong learning and (2) what kind of ethical issues are emerging with AI in education from the viewpoints of schools and other learning environments. The chapter first summarizes how recent AI technologies provide several options for learning and educational services and how AI is applied already in societies. In education and learning, many advanced techniques are already available, and we have tentatively promising findings. However, the accelerating pace of development of technology expands AI’s potentialities in education, so we need extensive new research about educational implementations and their effects on human learning and people’s lives. The chapter also summarizes how different chapters provide new research on AI in learning and education.},
	language = {en},
	urldate = {2023-03-30},
	booktitle = {{AI} in {Learning}: {Designing} the {Future}},
	publisher = {Springer International Publishing},
	author = {Niemi, Hannele and Pea, Roy D. and Lu, Yu},
	editor = {Niemi, Hannele and Pea, Roy D. and Lu, Yu},
	year = {2023},
	doi = {10.1007/978-3-031-09687-7_1},
	keywords = {AI ethical issues, Artificial intelligence, Human learning, Lifelong learning, Machine learning},
	pages = {1--15},
}

@article{rzepka_user_nodate,
	title = {User {Interaction} with {AI}-enabled {Systems}: {A} {Systematic} {Review} of {IS} {Research}},
	abstract = {The improved performance of technological capabilities in the field of artificial intelligence (AI), including computer vision and natural language processing, makes it possible to enhance existing and to develop new types of information systems. We refer to such systems as AI-enabled systems. User interaction with these systems is an important topic for information systems (IS) research because they are supposed to bring about substantial change for individuals, organizations, and society. Despite the recent public and academic interest in AI, AI-enabled systems are not a new phenomenon. However, previous research is separated into research streams on different AI-enabled system types. We conducted a literature review to aggregate the dispersed knowledge regarding individual user interaction with such systems in IS research. Our results show common behavioral patterns in interactions between users and various types of AIenabled systems and provide a solid foundation for future research on this topic.},
	language = {en},
	author = {Rzepka, Christine and Berger, Benedikt},
}

@article{glikson_human_2020,
	title = {Human {Trust} in {Artificial} {Intelligence}: {Review} of {Empirical} {Research}},
	volume = {14},
	issn = {1941-6520},
	shorttitle = {Human {Trust} in {Artificial} {Intelligence}},
	url = {https://journals.aom.org/doi/abs/10.5465/annals.2018.0057},
	doi = {10.5465/annals.2018.0057},
	abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
	number = {2},
	urldate = {2023-03-30},
	journal = {Academy of Management Annals},
	author = {Glikson, Ella and Woolley, Anita Williams},
	month = jul,
	year = {2020},
	note = {Publisher: Academy of Management},
	keywords = {Artificial Intelligence, human-computer interaction, human-machine interaction, human-robot interaction},
	pages = {627--660},
}

@inproceedings{zheng_ux_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {{UX} {Research} on {Conversational} {Human}-{AI} {Interaction}: {A} {Literature} {Review} of the {ACM} {Digital} {Library}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{UX} {Research} on {Conversational} {Human}-{AI} {Interaction}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501855},
	doi = {10.1145/3491102.3501855},
	abstract = {Early conversational agents (CAs) focused on dyadic human-AI interaction between humans and the CAs, followed by the increasing popularity of polyadic human-AI interaction, in which CAs are designed to mediate human-human interactions. CAs for polyadic interactions are unique because they encompass hybrid social interactions, i.e., human-CA, human-to-human, and human-to-group behaviors. However, research on polyadic CAs is scattered across different fields, making it challenging to identify, compare, and accumulate existing knowledge. To promote the future design of CA systems, we conducted a literature review of ACM publications and identified a set of works that conducted UX (user experience) research. We qualitatively synthesized the effects of polyadic CAs into four aspects of human-human interactions, i.e., communication, engagement, connection, and relationship maintenance. Through a mixed-method analysis of the selected polyadic and dyadic CA studies, we developed a suite of evaluation measurements on the effects. Our findings show that designing with social boundaries, such as privacy, disclosure, and identification, is crucial for ethical polyadic CAs. Future research should also advance usability testing methods and trust-building guidelines for conversational AI.},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Qingxiao and Tang, Yiliu and Liu, Yiren and Liu, Weizi and Huang, Yun},
	month = apr,
	year = {2022},
	keywords = {Chatbot, Conversational AI, Conversational Agent, Literature Review, UX Research},
	pages = {1--24},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{eloundou_gpts_2023,
	title = {{GPTs} are {GPTs}: {An} {Early} {Look} at the {Labor} {Market} {Impact} {Potential} of {Large} {Language} {Models}},
	shorttitle = {{GPTs} are {GPTs}},
	url = {http://arxiv.org/abs/2303.10130},
	doi = {10.48550/arXiv.2303.10130},
	abstract = {We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of GPTs, while around 19\% of workers may see at least 50\% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.},
	urldate = {2023-03-22},
	publisher = {arXiv},
	author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10130 [cs, econ, q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Economics - General Economics},
}

@article{zhong_exploration_2016,
	title = {An {Exploration} of {Three}-{Dimensional} {Integrated} {Assessment} for {Computational} {Thinking}},
	volume = {53},
	issn = {0735-6331, 1541-4140},
	url = {http://journals.sagepub.com/doi/10.1177/0735633115608444},
	doi = {10.1177/0735633115608444},
	abstract = {Computational thinking (CT) is a fundamental skill for students, and assessment is a critical factor in education. However, there is a lack of effective approaches to CT assessment. Therefore, we designed the Three-Dimensional Integrated Assessment (TDIA) framework in this article. The TDIA has two aims: one was to integrate three dimensions (directionality, openness, and process) into the design of effective assessment tasks; and the other was to assess comprehensively the three dimensions of CT including computational concepts, practices, and perspectives. Guided by the TDIA framework, we designed three pairs of tasks: closed forward tasks and closed reverse tasks, semiopen forward tasks and semiopen reverse tasks, and open tasks with a creative design report and open tasks without a creative design report. To further confirm each task’s applicability and its advantages and disadvantages, we conducted a test experiment at the end of the autumn semester in 2014 in a primary school for 3 weeks. The results indicated that (a) the reverse tasks were not more superior than the forward tasks; (b) the semiopen tasks and the open tasks were more effective than the closed tasks, and the semiopen tasks had higher difficulty and discrimination than the others; (c) the self-reports provided a helpful function for learning diagnosis and guidance; (d) the scores had no significant difference between the schoolboys and the schoolgirls in all six tasks; and (e) the six tasks’ difficulty and discrimination were all acceptable, and the semiopen tasks had higher difficulty and discrimination than the others. To effectively apply them, the following suggestions for teachers to design computational tasks are proposed: motivating students’ interest and enthusiasm, incorporating semifinished artifacts, involving learning diagnosis and guidance, and including multiple types of tasks in an assessment.},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Journal of Educational Computing Research},
	author = {Zhong, Baichang and Wang, Qiyun and Chen, Jie and Li, Yi},
	month = jan,
	year = {2016},
	pages = {562--590},
}

@article{kong_study_2018,
	title = {A study of primary school students' interest, collaboration attitude, and programming empowerment in computational thinking education},
	volume = {127},
	issn = {03601315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360131518302367},
	doi = {10.1016/j.compedu.2018.08.026},
	language = {en},
	urldate = {2023-03-21},
	journal = {Computers \& Education},
	author = {Kong, Siu-Cheung and Chiu, Ming Ming and Lai, Ming},
	month = dec,
	year = {2018},
	pages = {178--189},
}

@article{grover_computational_2013,
	title = {Computational {Thinking} in {K}–12: {A} {Review} of the {State} of the {Field}},
	volume = {42},
	issn = {0013-189X, 1935-102X},
	shorttitle = {Computational {Thinking} in {K}–12},
	url = {http://journals.sagepub.com/doi/10.3102/0013189X12463051},
	doi = {10.3102/0013189X12463051},
	abstract = {Jeannette Wing’s influential article on computational thinking 6 years ago argued for adding this new competency to every child’s analytical ability as a vital ingredient of science, technology, engineering, and mathematics (STEM) learning. What is computational thinking? Why did this article resonate with so many and serve as a rallying cry for educators, education researchers, and policy makers? How have they interpreted Wing’s definition, and what advances have been made since Wing’s article was published? This article frames the current state of discourse on computational thinking in K–12 education by examining mostly recently published academic literature that uses Wing’s article as a springboard, identifies gaps in research, and articulates priorities for future inquiries.},
	language = {en},
	number = {1},
	urldate = {2023-03-21},
	journal = {Educational Researcher},
	author = {Grover, Shuchi and Pea, Roy},
	month = jan,
	year = {2013},
	pages = {38--43},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_gpt-3_nodate,
	title = {{GPT}-3, {Bloviator}: {OpenAI}’s language generator has no idea what it’s talking about},
	shorttitle = {{GPT}-3, {Bloviator}},
	url = {https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/},
	abstract = {Tests show that the popular AI still has a poor grasp of reality.},
	language = {en},
	urldate = {2023-03-12},
	journal = {MIT Technology Review},
}

@misc{noauthor_openais_2020,
	title = {{OpenAI}'s {GPT}-3 {Language} {Model}: {A} {Technical} {Overview}},
	shorttitle = {{OpenAI}'s {GPT}-3 {Language} {Model}},
	url = {https://lambdalabs.com/blog/demystifying-gpt-3},
	abstract = {Chuan Li, PhD reviews  GPT-3, the new NLP model from OpenAI. This paper empirically shows that language model performance scales as a power-law with model size, datataset size, and the amount of computation.},
	language = {en},
	urldate = {2023-03-12},
	month = jun,
	year = {2020},
}

@misc{hendrycks_unsolved_2022,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	doi = {10.48550/arXiv.2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2023-03-11},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@book{reich_failure_2020,
	address = {Cambridge, MA},
	title = {Failure to {Disrupt}: {Why} {Technology} {Alone} {Can}’t {Transform} {Education}},
	isbn = {978-0-674-08904-4},
	shorttitle = {Failure to {Disrupt}},
	abstract = {Proponents of large-scale learning have boldly promised that technology can disrupt traditional approaches to schooling, radically accelerating learning and democratizing education. Much-publicized experiments, often underwritten by Silicon Valley entrepreneurs, have been launched at elite universities and in elementary schools in the poorest neighborhoods. Such was the excitement that, in 2012, the New York Times declared the “year of the MOOC.” Less than a decade later, that pronouncement seems premature.
In Failure to Disrupt: Why Technology Alone Can’t Transform Education, Justin Reich delivers a sobering report card on the latest supposedly transformative educational technologies. Reich takes readers on a tour of MOOCs, autograders, computerized “intelligent tutors,” and other educational technologies whose problems and paradoxes have bedeviled educators. Learning technologies—even those that are free to access—often provide the greatest benefit to affluent students and do little to combat growing inequality in education. And institutions and investors often favor programs that scale up quickly, but at the expense of true innovation. It turns out that technology cannot by itself disrupt education or provide shortcuts past the hard road of institutional change.
Technology does have a crucial role to play in the future of education, Reich concludes. We still need new teaching tools, and classroom experimentation should be encouraged. But successful reform efforts will focus on incremental improvements, not the next killer app.
Visit Justin Reich’s website for Failure to Disrupt, which features a schedule of speaking engagements and information about book club opportunities},
	publisher = {Harvard University Press},
	author = {Reich, Justin},
	month = sep,
	year = {2020},
}

@misc{susnjak_chatgpt_2022,
	title = {{ChatGPT}: {The} {End} of {Online} {Exam} {Integrity}?},
	shorttitle = {{ChatGPT}},
	url = {http://arxiv.org/abs/2212.09292},
	doi = {10.48550/arXiv.2212.09292},
	abstract = {This study evaluated the ability of ChatGPT, a recently developed artificial intelligence (AI) agent, to perform high-level cognitive tasks and produce text that is indistinguishable from human-generated text. This capacity raises concerns about the potential use of ChatGPT as a tool for academic misconduct in online exams. The study found that ChatGPT is capable of exhibiting critical thinking skills and generating highly realistic text with minimal input, making it a potential threat to the integrity of online exams, particularly in tertiary education settings where such exams are becoming more prevalent. Returning to invigilated and oral exams could form part of the solution, while using advanced proctoring techniques and AI-text output detectors may be effective in addressing this issue, they are not likely to be foolproof solutions. Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools. It is crucial for educators and institutions to be aware of the possibility of ChatGPT being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Susnjak, Teo},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{thunstrom_we_nodate,
	title = {We {Asked} {GPT}-3 to {Write} an {Academic} {Paper} about {Itself}\&mdash;{Then} {We} {Tried} to {Get} {It} {Published}},
	url = {https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/},
	abstract = {An artificially intelligent first author presents many ethical questions\&mdash;and could upend the publishing process},
	language = {en},
	urldate = {2023-03-09},
	journal = {Scientific American},
	author = {Thunström, Almira Osmanovic},
}

@article{stokel-walker_ai_2022,
	title = {{AI} bot {ChatGPT} writes smart essays — should professors worry?},
	copyright = {2022 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-022-04397-7},
	doi = {10.1038/d41586-022-04397-7},
	abstract = {The bot is free for now and can produce uncannily natural, well-referenced writing in response to homework questions.},
	language = {en},
	urldate = {2023-03-09},
	journal = {Nature},
	author = {Stokel-Walker, Chris},
	month = dec,
	year = {2022},
	note = {Bandiera\_abtest: a
Cg\_type: News Explainer
Publisher: Nature Publishing Group
Subject\_term: Computer science, Society, Education, Lab life},
	keywords = {Computer science, Education, Lab life, Society},
}

@techreport{ahmad_data-driven_2020,
	type = {preprint},
	title = {Data-{Driven} {Artificial} {Intelligence} in {Education}: {A} {Comprehensive} {Review}},
	shorttitle = {Data-{Driven} {Artificial} {Intelligence} in {Education}},
	url = {https://osf.io/zvu2n},
	abstract = {As Education constitutes an essential development standard for individuals and  societies,  researchers  have  been  exploring  the  use  of  Artificial Intelligence (AI) in this domain and have embedded the technology within it through a myriad of applications. In order to provide a detailed overview of the efforts, this article pays particular attention to these developments by highlighting key application areas of data-driven AI in Education;  it also analyzes existing tools, research  trends, as well as limitations of the role data-driven AI can play in Education\vphantom{\{}\}. In particular, the article reviews various applications of AI in Education including student grading and assessments, student retention and drop-out predictions, sentiment analysis, intelligent tutoring, classroom monitoring and recommender systems. The article also provides detailed bibliometric analysis to highlight the salient research trends in AI in Education over seven years (2014–2020) and further provides detailed description of the tools and platforms developed as the outcome of research and development efforts in AI and Education. For the bibliometric analysis, articles from several top venues are analyzed to explore research trends in the domain. The analysis shows sufficient contribution in the domain from different parts of the world with a clear lead for the United States. Moreover, students' grading and evaluation have been observed as the most widely explored application. Despite the significant success, we observed several aspects of education where AI alone can not contribute much. We believe such detailed analysis is expected to provide a baseline for future research in the domain.},
	language = {en},
	urldate = {2023-03-08},
	institution = {EdArXiv},
	author = {Ahmad, Kashif and Qadir, Junaid and Al-Fuqaha, Ala and Iqbal, Waleed and El-Hassan, Ammar and Benhaddou, Driss and Ayyash, Moussa},
	month = jun,
	year = {2020},
	doi = {10.35542/osf.io/zvu2n},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wu_promptchainer_2022,
	title = {{PromptChainer}: {Chaining} {Large} {Language} {Model} {Prompts} through {Visual} {Programming}},
	shorttitle = {{PromptChainer}},
	url = {http://arxiv.org/abs/2203.06566},
	abstract = {While LLMs have made it possible to rapidly prototype new ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains – a key step to lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We find from pilot studies that users need support transforming data between steps of a chain, as well as debugging the chain at multiple granularities. To address these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four designers and developers, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to even more complex tasks, as well as supporting low-fi chain prototyping.},
	language = {en},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.06566 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{lee_coauthor_2022,
	title = {{CoAuthor}: {Designing} a {Human}-{AI} {Collaborative} {Writing} {Dataset} for {Exploring} {Language} {Model} {Capabilities}},
	shorttitle = {{CoAuthor}},
	url = {http://arxiv.org/abs/2201.06796},
	doi = {10.1145/3491102.3502030},
	abstract = {Large language models (LMs) offer unprecedented language generation capabilities and exciting opportunities for interaction design. However, their highly context-dependent capabilities are difficult to grasp and are often subjectively interpreted. In this paper, we argue that by curating and analyzing large interaction datasets, the HCI community can foster more incisive examinations of LMs' generative capabilities. Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3's capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions. We demonstrate that CoAuthor can address questions about GPT-3's language, ideation, and collaboration capabilities, and reveal its contribution as a writing "collaborator" under various definitions of good collaboration. Finally, we discuss how this work may facilitate a more principled discussion around LMs' promises and pitfalls in relation to interaction design. The dataset and an interface for replaying the writing sessions are publicly available at https://coauthor.stanford.edu.},
	urldate = {2023-03-08},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Lee, Mina and Liang, Percy and Yang, Qian},
	month = apr,
	year = {2022},
	note = {arXiv:2201.06796 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	pages = {1--19},
}

@misc{coenen_wordcraft_2021,
	title = {Wordcraft: a {Human}-{AI} {Collaborative} {Editor} for {Story} {Writing}},
	shorttitle = {Wordcraft},
	url = {http://arxiv.org/abs/2107.07430},
	doi = {10.48550/arXiv.2107.07430},
	abstract = {As neural language models grow in effectiveness, they are increasingly being applied in real-world settings. However these applications tend to be limited in the modes of interaction they support. In this extended abstract, we propose Wordcraft, an AI-assisted editor for story writing in which a writer and a dialog system collaborate to write a story. Our novel interface uses few-shot learning and the natural affordances of conversation to support a variety of interactions. Our editor provides a sandbox for writers to probe the boundaries of transformer-based language models and paves the way for future human-in-the-loop training pipelines and novel evaluation methods.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Coenen, Andy and Davis, Luke and Ippolito, Daphne and Reif, Emily and Yuan, Ann},
	month = jul,
	year = {2021},
	note = {arXiv:2107.07430 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{manning_human_2022,
	title = {Human {Language} {Understanding} \& {Reasoning}},
	volume = {151},
	issn = {0011-5266, 1548-6192},
	url = {https://direct.mit.edu/daed/article/151/2/127/110621/Human-Language-Understanding-amp-Reasoning},
	doi = {10.1162/daed_a_01905},
	abstract = {Abstract
            The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
	language = {en},
	number = {2},
	urldate = {2023-03-08},
	journal = {Daedalus},
	author = {Manning, Christopher D.},
	month = may,
	year = {2022},
	pages = {127--138},
}

@misc{noauthor_risks_2021,
	title = {On {The} {Risks} of {Emergent} {Behavior} in {Foundation} {Models}},
	url = {https://bounded-regret.ghost.io/on-the-risks-of-emergent-behavior-in-foundation-models/},
	abstract = {This post first appeared as a commentary
[https://crfm.stanford.edu/2021/10/18/commentaries.html] for the paper "On The
Opportunities and Risks of Foundation Models".

Bommasani et al. (2021) [https://arxiv.org/abs/2108.07258] discuss a trend in
machine learning, whereby increasingly large-scale models are trained once},
	language = {en},
	urldate = {2023-03-08},
	journal = {Bounded Regret},
	month = oct,
	year = {2021},
}

@inproceedings{ganguli_predictability_2022,
	title = {Predictability and {Surprise} in {Large} {Generative} {Models}},
	url = {http://arxiv.org/abs/2202.07785},
	doi = {10.1145/3531146.3533229},
	abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
	urldate = {2023-03-08},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and Hatfield-Dodds, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
	month = jun,
	year = {2022},
	note = {arXiv:2202.07785 [cs]},
	keywords = {Computer Science - Computers and Society},
	pages = {1747--1764},
}

@misc{sun_recitation-augmented_2023,
	title = {Recitation-{Augmented} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.01296},
	abstract = {We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrievalaugmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE ﬁrst recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the ﬁnal answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Speciﬁcally, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at https://github.com/Edward-Sun/RECITE.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Sun, Zhiqing and Wang, Xuezhi and Tay, Yi and Yang, Yiming and Zhou, Denny},
	month = feb,
	year = {2023},
	note = {arXiv:2210.01296 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? \&\#x1f99c;},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@misc{wu_ai_2022,
	title = {{AI} {Chains}: {Transparent} and {Controllable} {Human}-{AI} {Interaction} by {Chaining} {Large} {Language} {Model} {Prompts}},
	shorttitle = {{AI} {Chains}},
	url = {http://arxiv.org/abs/2110.01691},
	doi = {10.48550/arXiv.2110.01691},
	abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by "unit-testing" sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie J.},
	month = mar,
	year = {2022},
	note = {arXiv:2110.01691 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@article{binz_using_2023,
	title = {Using cognitive psychology to understand {GPT}-3},
	volume = {120},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2218523120},
	doi = {10.1073/pnas.2218523120},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	number = {6},
	urldate = {2023-03-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Binz, Marcel and Schulz, Eric},
	month = feb,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2218523120},
}

@inproceedings{shakeri_saga_2021,
	address = {New York, NY, USA},
	series = {{CSCW} '21},
	title = {{SAGA}: {Collaborative} {Storytelling} with {GPT}-3},
	isbn = {978-1-4503-8479-7},
	shorttitle = {{SAGA}},
	url = {https://doi.org/10.1145/3462204.3481771},
	doi = {10.1145/3462204.3481771},
	abstract = {When friends live across different time zones, have incompatible work schedules, or have different levels of access to technology, synchronous communication becomes infeasible. To address this challenge, we developed a web application that allows friends to asynchronously collaborate creatively. In this application, multiple people can contribute to the writing of a story, told partially by a natural language AI system. By offloading some of the creative work to the AI, the human writers have the opportunity to also act as readers, being surprised by new events in the story. To gain preliminary insights into the experience of using this system, we conducted an informal pilot study over a span of 5 days. Through this process, we learned that storytelling with an AI system can encourage roleplay, it can be a cathartic experience, and it is curiosity-driven. Our recommendations for future research include (1) investigating new turn-taking strategies, and clearly communicating turns through the interface, (2) providing guidance for the prompt-writing process, perhaps through editable prompt templates, and (3) conducting a thorough evaluation of the system with friend groups of various sizes and timezones.},
	urldate = {2023-03-04},
	booktitle = {Companion {Publication} of the 2021 {Conference} on {Computer} {Supported} {Cooperative} {Work} and {Social} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Shakeri, Hanieh and Neustaedter, Carman and DiPaola, Steve},
	month = oct,
	year = {2021},
	keywords = {collaborative storytelling, distributed games, natural language processing, slow gameplay},
	pages = {163--166},
}

@article{guo_conditional_2021,
	title = {Conditional {Text} {Generation} for {Harmonious} {Human}-{Machine} {Interaction}},
	volume = {12},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3439816},
	doi = {10.1145/3439816},
	abstract = {In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
	number = {2},
	urldate = {2023-03-04},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
	month = feb,
	year = {2021},
	keywords = {Human-computer interaction, conditional text generation, deep learning, dialog systems, personalization},
	pages = {14:1--14:50},
}

@article{floridi_gpt-3_2020,
	title = {{GPT}-3: {Its} {Nature}, {Scope}, {Limits}, and {Consequences}},
	volume = {30},
	issn = {1572-8641},
	shorttitle = {{GPT}-3},
	url = {https://doi.org/10.1007/s11023-020-09548-1},
	doi = {10.1007/s11023-020-09548-1},
	abstract = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.},
	language = {en},
	number = {4},
	urldate = {2023-03-04},
	journal = {Minds and Machines},
	author = {Floridi, Luciano and Chiriatti, Massimo},
	month = dec,
	year = {2020},
	pages = {681--694},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	doi = {10.48550/arXiv.2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kojima_large_2023,
	title = {Large {Language} {Models} are {Zero}-{Shot} {Reasoners}},
	url = {http://arxiv.org/abs/2205.11916},
	doi = {10.48550/arXiv.2205.11916},
	abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	month = jan,
	year = {2023},
	note = {arXiv:2205.11916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-03-04},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_prompt_2023,
	title = {Prompt {Engineering} 101: {Introduction} and resources},
	shorttitle = {Prompt {Engineering} 101},
	url = {https://amatriain.net/blog/PromptEngineering},
	abstract = {TOC},
	language = {en},
	urldate = {2023-03-04},
	journal = {AI, software, tech, and people, not in that order... by X},
	month = jan,
	year = {2023},
}

@misc{levy_assessing_2021,
	title = {Assessing the {Impact} of {Automated} {Suggestions} on {Decision} {Making}: {Domain} {Experts} {Mediate} {Model} {Errors} but {Take} {Less} {Initiative}},
	shorttitle = {Assessing the {Impact} of {Automated} {Suggestions} on {Decision} {Making}},
	url = {http://arxiv.org/abs/2103.04725},
	doi = {10.48550/arXiv.2103.04725},
	abstract = {Automated decision support can accelerate tedious tasks as users can focus their attention where it is needed most. However, a key concern is whether users overly trust or cede agency to automation. In this paper, we investigate the effects of introducing automation to annotating clinical texts--a multi-step, error-prone task of identifying clinical concepts (e.g., procedures) in medical notes, and mapping them to labels in a large ontology. We consider two forms of decision aid: recommending which labels to map concepts to, and pre-populating annotation suggestions. Through laboratory studies, we find that 18 clinicians generally build intuition of when to rely on automation and when to exercise their own judgement. However, when presented with fully pre-populated suggestions, these expert users exhibit less agency: accepting improper mentions, and taking less initiative in creating additional annotations. Our findings inform how systems and algorithms should be designed to mitigate the observed issues.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Levy, Ariel and Agrawal, Monica and Satyanarayan, Arvind and Sontag, David},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04725 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{davis_empirically_2016,
	address = {New York, NY, USA},
	series = {{IUI} '16},
	title = {Empirically {Studying} {Participatory} {Sense}-{Making} in {Abstract} {Drawing} with a {Co}-{Creative} {Cognitive} {Agent}},
	isbn = {978-1-4503-4137-0},
	url = {https://doi.org/10.1145/2856767.2856795},
	doi = {10.1145/2856767.2856795},
	abstract = {This paper reports on the design and evaluation of a co-creative drawing partner called the Drawing Apprentice, which was designed to improvise and collaborate on abstract sketches with users in real time. The system qualifies as a new genre of creative technologies termed "casual creators" that are meant to creatively engage users and provide enjoyable creative experiences rather than necessarily helping users make a higher quality creative product. We introduce the conceptual framework of participatory sense-making and describe how it can help model and understand open-ended collaboration. We report the results of a user study comparing human-human collaboration to human-computer collaboration using the Drawing Apprentice system. Based on insights from the user study, we present a set of design recommendations for co-creative agents.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Davis, Nicholas and Hsiao, Chih-PIn and Yashraj Singh, Kunwar and Li, Lisa and Magerko, Brian},
	month = mar,
	year = {2016},
	keywords = {collaboration, computational creativity, creativity support tools},
	pages = {196--207},
}

@misc{huang_ai_2020,
	title = {{AI} {Song} {Contest}: {Human}-{AI} {Co}-{Creation} in {Songwriting}},
	shorttitle = {{AI} {Song} {Contest}},
	url = {http://arxiv.org/abs/2010.05388},
	doi = {10.48550/arXiv.2010.05388},
	abstract = {Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation, or algorithmically ranked the samples. Ultimately, teams not only had to manage the "flare and focus" aspects of the creative process, but also juggle them with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Huang, Cheng-Zhi Anna and Koops, Hendrik Vincent and Newton-Rex, Ed and Dinculescu, Monica and Cai, Carrie J.},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05388 [cs, eess]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, I.2, J.5},
}

@misc{buschek_nine_2021,
	title = {Nine {Potential} {Pitfalls} when {Designing} {Human}-{AI} {Co}-{Creative} {Systems}},
	url = {http://arxiv.org/abs/2104.00358},
	doi = {10.48550/arXiv.2104.00358},
	abstract = {This position paper examines potential pitfalls on the way towards achieving human-AI co-creation with generative models in a way that is beneficial to the users' interests. In particular, we collected a set of nine potential pitfalls, based on the literature and our own experiences as researchers working at the intersection of HCI and AI. We illustrate each pitfall with examples and suggest ideas for addressing it. Reflecting on all pitfalls, we discuss and conclude with implications for future research directions. With this collection, we hope to contribute to a critical and constructive discussion on the roles of humans and AI in co-creative interactions, with an eye on related assumptions and potential side-effects for creative practices and beyond.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Buschek, Daniel and Mecke, Lukas and Lehmann, Florian and Dang, Hai},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00358 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, H.5.2, I.2.m},
}

@inproceedings{amershi_guidelines_2019,
	address = {Glasgow Scotland Uk},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300233},
	doi = {10.1145/3290605.3300233},
	abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for humanAI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
	month = may,
	year = {2019},
	pages = {1--13},
}

@misc{noauthor_metaphoria_nodate,
	title = {Metaphoria {\textbar} {Proceedings} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300526},
	urldate = {2023-02-28},
}

@inproceedings{clark_creative_2018,
	address = {New York, NY, USA},
	series = {{IUI} '18},
	title = {Creative {Writing} with a {Machine} in the {Loop}: {Case} {Studies} on {Slogans} and {Stories}},
	isbn = {978-1-4503-4945-1},
	shorttitle = {Creative {Writing} with a {Machine} in the {Loop}},
	url = {https://doi.org/10.1145/3172944.3172983},
	doi = {10.1145/3172944.3172983},
	abstract = {As the quality of natural language generated by artificial intelligence systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed two case studies using two system prototypes, one for short story writing and one for slogan writing. Participants in our studies were asked to write with a machine in the loop or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, machine suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design choices that may better support creative writing.},
	urldate = {2023-02-28},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji, Yangfeng and Smith, Noah A.},
	month = mar,
	year = {2018},
	keywords = {creative writing, machine in the loop, natural language processing},
	pages = {329--340},
}

@inproceedings{koch_may_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {May {AI}? {Design} {Ideation} with {Cooperative} {Contextual} {Bandits}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {May {AI}?},
	url = {https://doi.org/10.1145/3290605.3300863},
	doi = {10.1145/3290605.3300863},
	abstract = {Design ideation is a prime creative activity in design. However, it is challenging to support computationally due to its quickly evolving and exploratory nature. The paper presents cooperative contextual bandits (CCB) as a machine-learning method for interactive ideation support. A CCB can learn to propose domain-relevant contributions and adapt their exploration/exploitation strategy. We developed a CCB for an interactive design ideation tool that 1) suggests inspirational and situationally relevant materials ("may AI?"); 2) explores and exploits inspirational materials with the designer; and 3) explains its suggestions to aid reflection. The application case of digital mood board design is presented, wherein visual inspirational materials are collected and curated in collages. In a controlled study, 14 of 16 professional designers preferred the CCB-augmented tool. The CCB approach holds promise for ideation activities wherein adaptive and steerable support is welcome but designers must retain full outcome control.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Koch, Janin and Lucero, Andrés and Hegemann, Lena and Oulasvirta, Antti},
	month = may,
	year = {2019},
	keywords = {creativity support tools, ideation support, interactive machine-learning, mood board design},
	pages = {1--12},
}

@inproceedings{schleith_cognitive_2022,
	address = {New York, NY, USA},
	series = {C\&amp;{C} '22},
	title = {Cognitive {Strategy} {Prompts}: {Creativity} {Triggers} for {Human} {Centered} {AI} {Opportunity} {Detection}},
	isbn = {978-1-4503-9327-0},
	shorttitle = {Cognitive {Strategy} {Prompts}},
	url = {https://doi.org/10.1145/3527927.3532808},
	doi = {10.1145/3527927.3532808},
	abstract = {Creative problem solving and innovation powered by Artificial Intelligence (AI) requires detection of user needs that can be reframed into data science problems. We propose a framework of 10 creativity triggers for creative human centered AI opportunity detection, based on research and categorization of information retrieval tasks and cognitive task analysis. The method aims to facilitate a dialog between data scientists and underrepresented groups such as non-technical domain experts. Impact on problem discovery and idea generation was evaluated in co-creation workshops. Results show that the method significantly increases ideas’ scores on the appropriateness to a specific problem and their AI relevancy. Participants experienced the prompts as a helpful mental framework about AI methods and felt encouraged to decompose user stories into more detailed cognitive tasks that help data scientists relate ideas to high level data science methods.},
	urldate = {2023-02-28},
	booktitle = {Creativity and {Cognition}},
	publisher = {Association for Computing Machinery},
	author = {Schleith, Johannes and Norkute, Milda and Mikhail, Mary and Tsar, Daniella},
	month = jun,
	year = {2022},
	keywords = {AI opportunity detection, cognitive task analysis, design thinking},
	pages = {29--37},
}

@inproceedings{wu_ai_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {{AI} {Chains}: {Transparent} and {Controllable} {Human}-{AI} {Interaction} by {Chaining} {Large} {Language} {Model} {Prompts}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{AI} {Chains}},
	url = {https://doi.org/10.1145/3491102.3517582},
	doi = {10.1145/3491102.3517582},
	abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
	month = apr,
	year = {2022},
	keywords = {Human-AI Interaction, Large Language Models, Natural Language Processing},
	pages = {1--22},
}

@inproceedings{meyer_we_2022,
	address = {New York, NY, USA},
	series = {{CUI} '22},
	title = {Do {We} {Still} {Need} {Human} {Assessors}? {Prompt}-{Based} {GPT}-3 {User} {Simulation} in {Conversational} {AI}},
	isbn = {978-1-4503-9739-1},
	shorttitle = {Do {We} {Still} {Need} {Human} {Assessors}?},
	url = {https://doi.org/10.1145/3543829.3544529},
	doi = {10.1145/3543829.3544529},
	abstract = {Scarcity of user data continues to be a problem in research on conversational user interfaces and often hinders or slows down technical innovation. In the past, different ways of synthetically generating data, such as data augmentation techniques have been explored. With the rise of ever improving pre-trained language models, we ask if we can go beyond such methods by simply providing appropriate prompts to these general purpose models to generate data. We explore the feasibility and cost-benefit trade-offs of using non fine-tuned synthetic data to train classification algorithms for conversational agents. We compare this synthetically generated data with real user data and evaluate the performance of classifiers trained on different combinations of synthetic and real data. We come to the conclusion that, although classifiers trained on such synthetic data perform much better than random baselines, they do not compare to the performance of classifiers trained on even very small amounts of real user data, largely because such data is lacking much of the variability found in user generated data. Nevertheless, we show that in situations where very little data and resources are available, classifiers trained on such synthetically generated data might be preferable to the collection and annotation of naturalistic data.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 4th {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Meyer, Selina and Elsweiler, David and Ludwig, Bernd and Fernandez-Pichel, Marcos and Losada, David E.},
	month = sep,
	year = {2022},
	keywords = {conversational ai, datasets, nlp, text generation},
	pages = {1--6},
}

@inproceedings{lee_promptiverse_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Promptiverse: {Scalable} {Generation} of {Scaffolding} {Prompts} {Through} {Human}-{AI} {Hybrid} {Knowledge} {Graph} {Annotation}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Promptiverse},
	url = {https://doi.org/10.1145/3491102.3502087},
	doi = {10.1145/3491102.3502087},
	abstract = {Online learners are hugely diverse with varying prior knowledge, but most instructional videos online are created to be one-size-fits-all. Thus, learners may struggle to understand the content by only watching the videos. Providing scaffolding prompts can help learners overcome these struggles through questions and hints that relate different concepts in the videos and elicit meaningful learning. However, serving diverse learners would require a spectrum of scaffolding prompts, which incurs high authoring effort. In this work, we introduce Promptiverse, an approach for generating diverse, multi-turn scaffolding prompts at scale, powered by numerous traversal paths over knowledge graphs. To facilitate the construction of the knowledge graphs, we propose a hybrid human-AI annotation tool, Grannotate. In our study (N=24), participants produced 40 times more on-par quality prompts with higher diversity, through Promptiverse and Grannotate, compared to hand-designed prompts. Promptiverse presents a model for creating diverse and adaptive learning experiences online.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Yoonjoo and Chung, John Joon Young and Kim, Tae Soo and Song, Jean Y and Kim, Juho},
	month = apr,
	year = {2022},
	keywords = {Scaffolding prompt, human-AI hybrid annotation, knowledge graph},
	pages = {1--18},
}

@misc{webson_prompt-based_2022,
	title = {Do {Prompt}-{Based} {Models} {Really} {Understand} the {Meaning} of their {Prompts}?},
	url = {http://arxiv.org/abs/2109.01247},
	doi = {10.48550/arXiv.2109.01247},
	abstract = {Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively "good" prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Webson, Albert and Pavlick, Ellie},
	month = apr,
	year = {2022},
	note = {arXiv:2109.01247 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{peters_knowledge_2019,
	title = {Knowledge {Enhanced} {Contextual} {Word} {Representations}},
	url = {http://arxiv.org/abs/1909.04164},
	abstract = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we ﬁrst use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and selfsupervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.},
	language = {en},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Peters, Matthew E. and Neumann, Mark and Logan IV, Robert L. and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A.},
	month = oct,
	year = {2019},
	note = {arXiv:1909.04164 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_everything_2022,
	title = {Everything you need to know about {Few}-{Shot} {Learning}},
	url = {https://blog.paperspace.com/few-shot-learning/},
	abstract = {In this tutorial, we examine the Few-Shot Learning paradigm for deep and machine learning tasks. Readers can expect to learn what it is, different techniques, and details about use cases for Few-Shot Learning},
	language = {en},
	urldate = {2023-02-28},
	journal = {Paperspace Blog},
	month = jun,
	year = {2022},
}

@misc{wang_entailment_2021,
	title = {Entailment as {Few}-{Shot} {Learner}},
	url = {http://arxiv.org/abs/2104.14690},
	doi = {10.48550/arXiv.2104.14690},
	abstract = {Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12{\textbackslash}\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
	month = apr,
	year = {2021},
	note = {arXiv:2104.14690 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhong_adapting_2021,
	title = {Adapting {Language} {Models} for {Zero}-shot {Learning} by {Meta}-tuning on {Dataset} and {Prompt} {Collections}},
	url = {http://arxiv.org/abs/2104.04670},
	doi = {10.48550/arXiv.2104.04670},
	abstract = {Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can "prompt" the LM with the review and the label description "Does the user like this movie?", and ask whether the next word is "yes" or "no". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3\%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
	month = sep,
	year = {2021},
	note = {arXiv:2104.04670 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{tam_improving_2021,
	title = {Improving and {Simplifying} {Pattern} {Exploiting} {Training}},
	url = {http://arxiv.org/abs/2103.11955},
	doi = {10.48550/arXiv.2103.11955},
	abstract = {Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data. Our code can be found at https://github.com/rrmenon10/ADAPET.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Tam, Derek and Menon, Rakesh R. and Bansal, Mohit and Srivastava, Shashank and Raffel, Colin},
	month = sep,
	year = {2021},
	note = {arXiv:2103.11955 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{talmor_olmpics_2020,
	title = {{oLMpics} -- {On} what {Language} {Model} {Pre}-training {Captures}},
	url = {http://arxiv.org/abs/1912.13283},
	doi = {10.48550/arXiv.1912.13283},
	abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
	month = nov,
	year = {2020},
	note = {arXiv:1912.13283 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{jiang_how_2020,
	title = {How {Can} {We} {Know} {What} {Language} {Models} {Know}?},
	url = {http://arxiv.org/abs/1911.12543},
	doi = {10.48550/arXiv.1911.12543},
	abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a \_ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a \_" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
	month = may,
	year = {2020},
	note = {arXiv:1911.12543 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{feldman_commonsense_2019,
	title = {Commonsense {Knowledge} {Mining} from {Pretrained} {Models}},
	url = {http://arxiv.org/abs/1909.00505},
	doi = {10.48550/arXiv.1909.00505},
	abstract = {Inferring commonsense knowledge is a key challenge in natural language processing, but due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple's validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though this method performs worse on a test set than models explicitly trained on a corresponding training set, it outperforms these methods when mining commonsense knowledge from new sources, suggesting that unsupervised techniques may generalize better than current supervised approaches.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Feldman, Joshua and Davison, Joe and Rush, Alexander M.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.00505 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{petroni_language_2019,
	title = {Language {Models} as {Knowledge} {Bases}?},
	url = {http://arxiv.org/abs/1909.01066},
	doi = {10.48550/arXiv.1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv:1909.01066 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_prompting_2021,
	title = {Prompting: {Better} {Ways} of {Using} {Language} {Models} for {NLP} {Tasks}},
	shorttitle = {Prompting},
	url = {https://thegradient.pub/prompting/},
	abstract = {A review of recent advances in prompts.},
	language = {en},
	urldate = {2023-02-28},
	journal = {The Gradient},
	month = jul,
	year = {2021},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	doi = {10.48550/arXiv.1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{reynolds_prompt_2021,
	title = {Prompt {Programming} for {Large} {Language} {Models}: {Beyond} the {Few}-{Shot} {Paradigm}},
	shorttitle = {Prompt {Programming} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2102.07350},
	doi = {10.48550/arXiv.2102.07350},
	abstract = {Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Reynolds, Laria and McDonell, Kyle},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07350 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{christian_alignment_2021,
	title = {The {Alignment} {Problem}: {Machine} {Learning} and {Human} {Values}},
	volume = {73},
	issn = {0892-2675, 0892-2675},
	shorttitle = {The {Alignment} {Problem}},
	url = {https://a916407.fmphost.com/fmi/webd/ASAdb49?script=doi-layout&$SearchString=https://doi.org/10.56315/PSCF12-21Christian},
	doi = {10.56315/PSCF12-21Christian},
	abstract = {THE ALIGNMENT PROBLEM: Machine Learning and Human Values by Brian Christian. New York: W. W. Norton, 2020. 344 pages. Hardcover; \$28.95. ISBN: 9780393635829. *The global conversation about artificial intelligence (AI) is increasingly polemic--"AI will change the world!" "AI will ruin the world!" Amidst the strife, Brian Christian's work stands out. It is thoughtful, nuanced, and, at times, even poetic. Coming on the heels of his two other bestsellers, The Most Human Human and Algorithms to Live By, this meticulously researched recounting of the last decade of research into AI safety provides a broad perspective of the field and its future. *The "alignment problem" in the title refers to the disconnect between what AI does and what we want it to do. In Christian's words, it is the disconnect between "machine learning and human values." This disconnect has been the subject of intense research in recent years, as both companies and academics continually discover that AIs inherit the mistakes and biases of their creators. *For example, we train AIs that predict recidivism rates of convicted criminals in hopes of crafting more accurate sentences. However, the AIs produce racially biased outcomes. Or, we train AIs which map words into mathematical spaces. These AIs can perform mathematical "computations" on words, such as "king - man + woman = queen" and "Paris - France + Italy = Rome." But they also say that "doctor - man + woman = nurse" and "computer programmer - man + woman = homemaker." These examples of racial and gender bias are some of the numerous ways that human bias appears inside the supposedly impartial tools we have created. *As Norbert Wiener, a famous mathematician in the mid-twentieth century, put it, "We had better be sure the purpose put into the machine is the purpose which we really desire" (p. 312). The discoveries of the last ten years have shocked researchers into realizing that our machines have purposes we never intended. Christian's message is clear: these mistakes must be fixed before those machines become a fixed part of our everyday lives. *The book is divided into three main sections. The first, Prophecy, provides a historical overview of how researchers uncovered the AI biases that are now well known. It traces the origins of how AI models ended up in the public sphere and the history of how people have tried to solve the problems AI creates. Perhaps one of the most interesting anecdotes in this section is about how researchers try to create explainable models to comply with GDPR requirements. *The second section, Agency, explores the alignment problem in the context of reinforcement learning. Reinforcement learning involves teaching computer "agents" (aka AIs) to perform certain tasks using complex reward systems. Time and time again, the reward systems that researchers create have unintended side effects, and Christian recounts numerous humorous examples of this. He explains in simple terms why it is so difficult to correctly motivate the behaviors we wish to see in others (both humans and machines), and what it might take to create machines which are truly curious. This section feels a bit long. Christian dives deeply into the research of a few specific labs and appears to lose his logical thread in the weeds of research. Eventually, he emerges. *The final section, Normativity, provides perspective on current efforts to understand and fix the alignment problem. Its subchapters, "Imitation," "Inference," and "Uncertainty," reference different qualities that human researchers struggle to instill in machines. Imitating correct behaviors while ignoring bad ones is hard, as is getting a machine to perform correctly on data it hasn't seen before. Finally, teaching a model (and humans reading its results) to correctly interpret uncertainty is an active area of research with no concrete solutions. *After spending over three hundred pages recounting the pitfalls of AI and the difficulties of realigning models with human values, Christian ends on a hopeful note. He postulates that the issues discovered in machine-learning models illuminate societal issues that might otherwise be ignored. "Unfair pretrial detection models, for one thing, shine a spotlight on upstream inequities. Biased language models give us, among other things, a way to measure the state of our discourse and offer us a benchmark against which to try to improve and better ourselves ... In seeing a kind of mind at work as it digests and reacts to the world, we will learn something both about the world and also, perhaps, about minds" (p. 328). *As a Christ-follower, I believe the biases found in AI are both terrible and unsurprising. Humans are imperfect creators. While researchers' efforts to fix biases and shortcomings in AI systems are important and worthwhile, they can never exorcise fallen human nature from AI. Christian's conclusions about AI pointing to biases in humans comes close to this idea but avoids taking an overtly theological stance. *This book is well worth reading for those who wish to better understand the limitations of AI and current efforts to fix them. It weaves together history, mathematics, ethics, and philosophy, while remaining accessible to a broad audience through smooth explanations of detailed concepts. You don't need to be an AI expert (or even familiar with AI at all) to appreciate this book's insights. *After you're done reading it, recommend this book to the next person who tells you, with absolute certainty, that AI will either save or ruin the world. Christian's book provides a much-needed dose of sanity and perspective amidst the hype. *Reviewed by Emily Wenger, graduate student in the Department of Computer Science, University of Chicago, Chicago, IL 60637.},
	language = {en},
	number = {4},
	urldate = {2023-02-24},
	journal = {Perspectives on Science and Christian Faith},
	author = {Christian, Brian},
	month = dec,
	year = {2021},
	pages = {245--247},
}

@misc{noauthor_ethics_nodate,
	title = {‪{The} ethics of artificial intelligence‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=es&user=oQwpz3QAAAAJ&citation_for_view=oQwpz3QAAAAJ:qUcmZB5y_30C},
	abstract = {‪N Bostrom, E Yudkowsky‬, ‪Artificial intelligence safety and security, 2018‬ - ‪Citado por 1.050‬},
	urldate = {2023-02-24},
}

@book{bostrom_superintelligence_2017,
	title = {Superintelligence},
	publisher = {Dunod},
	author = {Bostrom, Nick},
	year = {2017},
}

@misc{zhou_large_2022,
	title = {Large {Language} {Models} {Are} {Human}-{Level} {Prompt} {Engineers}},
	url = {http://arxiv.org/abs/2211.01910},
	doi = {10.48550/arXiv.2211.01910},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01910 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{butlin_ai_2021,
	address = {New York, NY, USA},
	series = {{AIES} '21},
	title = {{AI} {Alignment} and {Human} {Reward}},
	isbn = {978-1-4503-8473-5},
	url = {https://doi.org/10.1145/3461702.3462570},
	doi = {10.1145/3461702.3462570},
	abstract = {According to a prominent approach to AI alignment, AI agents should be built to learn and promote human values. However, humans value things in several different ways: we have desires and preferences of various kinds, and if we engage in reinforcement learning, we also have reward functions. One research project to which this approach gives rise is therefore to say which of these various classes of human values should be promoted. This paper takes on part of this project by assessing the proposal that human reward functions should be the target for AI alignment. There is some reason to believe that powerful AI agents which were aligned to values of this form would help us to lead good lives, but there is also considerable uncertainty about this claim, arising from unresolved empirical and conceptual issues in human psychology.},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Butlin, Patrick},
	month = jul,
	year = {2021},
	keywords = {human values, reward functions, value alignment, value learning},
	pages = {437--445},
}

@article{gabriel_artificial_2020,
	title = {Artificial {Intelligence}, {Values}, and {Alignment}},
	volume = {30},
	issn = {0924-6495, 1572-8641},
	url = {https://link.springer.com/10.1007/s11023-020-09539-2},
	doi = {10.1007/s11023-020-09539-2},
	abstract = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify ‘true’ moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people’s moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
	language = {en},
	number = {3},
	urldate = {2023-02-24},
	journal = {Minds and Machines},
	author = {Gabriel, Iason},
	month = sep,
	year = {2020},
	pages = {411--437},
}

@article{yudkowsky_ai_nodate,
	title = {The {AI} {Alignment} {Problem}: {Why} {It}’s {Hard}, and {Where} to {Start}},
	language = {en},
	author = {Yudkowsky, Eliezer},
}

@misc{eckersley_impossibility_2019,
	title = {Impossibility and {Uncertainty} {Theorems} in {AI} {Value} {Alignment} (or why your {AGI} should not have a utility function)},
	url = {http://arxiv.org/abs/1901.00064},
	doi = {10.48550/arXiv.1901.00064},
	abstract = {Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Eckersley, Peter},
	month = mar,
	year = {2019},
	note = {arXiv:1901.00064 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{noauthor_answers_nodate,
	title = {Answers to {Minna}'s competition},
}

@inproceedings{weisz_hai-gen_2022,
	address = {New York, NY, USA},
	series = {{IUI} '22 {Companion}},
	title = {{HAI}-{GEN} 2022: 3rd {Workshop} on {Human}-{AI} {Co}-{Creation} with {Generative} {Models}},
	isbn = {978-1-4503-9145-0},
	shorttitle = {{HAI}-{GEN} 2022},
	url = {https://doi.org/10.1145/3490100.3511166},
	doi = {10.1145/3490100.3511166},
	abstract = {Recent advances in generative AI have resulted in a rapid and dramatic increase to the fidelity of created artifacts, from realistic-looking images of faces [9] to antimicrobial peptide sequences that treat diseases [5] to faked videos of prominent business leaders [4, 10]. We believe that people skilled within their creative domain can realize great benefits by incorporating generative models into their own work: as a source of inspiration, as a tool for manipulation, or as a creative partner. Our workshop will bring together researchers and practitioners from both the HCI and AI disciplines to explore and better understand the opportunities and challenges in building, using, and evaluating human-AI co-creative systems.},
	urldate = {2023-02-24},
	booktitle = {27th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Weisz, Justin D. and Maher, Mary Lou and Strobelt, Hendrik and Chilton, Lydia B. and Bau, David and Geyer, Werner},
	month = mar,
	year = {2022},
	keywords = {Generative modelling, artificial intelligence, collaboration, creativity, generative design, user experience},
	pages = {4--6},
}

@inproceedings{louie_novice-ai_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Novice-{AI} {Music} {Co}-{Creation} via {AI}-{Steering} {Tools} for {Deep} {Generative} {Models}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376739},
	doi = {10.1145/3313831.3376739},
	abstract = {While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needfinding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its non-deterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy/sad, conventional/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users' trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efficacy and ownership of the composition relative to the AI.},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J.},
	month = apr,
	year = {2020},
	keywords = {co-creation, generative deep neural networks, human-ai interaction},
	pages = {1--13},
}

@inproceedings{suh_ai_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {{AI} as {Social} {Glue}: {Uncovering} the {Roles} of {Deep} {Generative} {AI} during {Social} {Music} {Composition}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{AI} as {Social} {Glue}},
	url = {https://doi.org/10.1145/3411764.3445219},
	doi = {10.1145/3411764.3445219},
	abstract = {Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with human beings in co-creating novel content (e.g. music, art). While substantial research focuses on (individual) human-AI collaborations, comparatively less research examines how AI can play a role in human-human collaborations during co-creation. In a qualitative lab study, we observed 30 participants (15 pairs) compose a musical phrase in pairs, both with and without AI. Our findings reveal that AI may play important roles in influencing human social dynamics during creativity, including: 1) implicitly seeding a common ground at the start of collaboration, 2) acting as a psychological safety net in creative risk-taking, 3) providing a force for group progress, 4) mitigating interpersonal stalling and friction, and 5) altering users’ collaborative and creative roles. This work contributes to the future of generative AI in social creativity by providing implications for how AI could enrich, impede, or alter creative social dynamics in the years to come.},
	urldate = {2023-02-24},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Suh, Minhyang (Mia) and Youngblom, Emily and Terry, Michael and Cai, Carrie J},
	month = may,
	year = {2021},
	keywords = {human-AI co-creation, machine learning, music composition},
	pages = {1--11},
}

@inproceedings{sun_investigating_2022,
	address = {New York, NY, USA},
	series = {{IUI} '22},
	title = {Investigating {Explainability} of {Generative} {AI} for {Code} through {Scenario}-based {Design}},
	isbn = {978-1-4503-9144-3},
	url = {https://doi.org/10.1145/3490099.3511119},
	doi = {10.1145/3490099.3511119},
	abstract = {What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.},
	urldate = {2023-02-24},
	booktitle = {27th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Jiao and Liao, Q. Vera and Muller, Michael and Agarwal, Mayank and Houde, Stephanie and Talamadupula, Kartik and Weisz, Justin D.},
	month = mar,
	year = {2022},
	keywords = {explainable AI, generative AI, human-centered AI, scenario based design, software engineering tooling},
	pages = {212--228},
}

@misc{houde_business_2020,
	title = {Business (mis){Use} {Cases} of {Generative} {AI}},
	url = {http://arxiv.org/abs/2003.07679},
	doi = {10.48550/arXiv.2003.07679},
	abstract = {Generative AI is a class of machine learning technology that learns to generate new data from training data. While deep fakes and media-and art-related generative AI breakthroughs have recently caught people's attention and imagination, the overall area is in its infancy for business use. Further, little is known about generative AI's potential for malicious misuse at large scale. Using co-creation design fictions with AI engineers, we explore the plausibility and severity of business misuse cases.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Houde, Stephanie and Liao, Vera and Martino, Jacquelyn and Muller, Michael and Piorkowski, David and Richards, John and Weisz, Justin and Zhang, Yunfeng},
	month = mar,
	year = {2020},
	note = {arXiv:2003.07679 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{gruetzemacher_forecasting_2019,
	title = {Forecasting {Transformative} {AI}: {An} {Expert} {Survey}},
	shorttitle = {Forecasting {Transformative} {AI}},
	url = {http://arxiv.org/abs/1901.08579},
	doi = {10.48550/arXiv.1901.08579},
	abstract = {Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5\% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40\% in 5 years and 60\% in 10 years. Median forecasts indicated a 50\% probability of AI systems being capable of automating 90\% of current human tasks in 25 years and 99\% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Gruetzemacher, Ross and Paradice, David and Lee, Kang Bok},
	month = jul,
	year = {2019},
	note = {arXiv:1901.08579 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{gruetzemacher_transformative_2022,
	title = {The transformative potential of artificial intelligence},
	volume = {135},
	issn = {0016-3287},
	url = {https://www.sciencedirect.com/science/article/pii/S0016328721001932},
	doi = {10.1016/j.futures.2021.102884},
	abstract = {The terms ‘human-level artificial intelligence’ and ‘artificial general intelligence’ are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term ‘transformative AI’ is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be ‘transformative’ is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.},
	language = {en},
	urldate = {2023-02-24},
	journal = {Futures},
	author = {Gruetzemacher, Ross and Whittlestone, Jess},
	month = jan,
	year = {2022},
	keywords = {Artificial Intelligence, Artificial general intelligence, Human-level AI, Transformative AI},
	pages = {102884},
}

@article{gruetzemacher_defining_nodate,
	title = {Defining and {Unpacking} {Transformative} {AI}},
	abstract = {Recently the concept of transformative AI (TAI) has begun to receive attention in the AI policy space. TAI is often framed as an alternative formulation to notions of strong AI (e.g. artificial general intelligence or superintelligence) and reflects increasing consensus that advanced AI which does not fit these definitions may nonetheless have extreme and long-lasting impacts on society. However, the term TAI is poorly defined and often used ambiguously. Some use the notion of TAI to describe levels of societal transformation associated with previous ‘general purpose technologies’ (GPTs) such as electricity or the internal combustion engine. Others use the term to refer to more drastic levels of transformation comparable to the agricultural or industrial revolutions. The notion has also been used much more loosely, with some implying that current AI systems are already having a transformative impact on society.},
	language = {en},
	author = {Gruetzemacher, Ross and Whittlestone, Jess},
}

@misc{noauthor_our_2022,
	title = {Our {Approach} to {Alignment} {Research}},
	url = {https://openai.com/blog/our-approach-to-alignment-research/},
	abstract = {Our approach to aligning AGI is empirical and iterative. We are improving our AI systems’ ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignment problems.




Introduction


Our alignment},
	language = {en},
	urldate = {2023-02-23},
	journal = {OpenAI},
	month = aug,
	year = {2022},
}

@misc{noauthor_forecasting_2023,
	title = {Forecasting {Potential} {Misuses} of {Language} {Models} for {Disinformation} {Campaigns}—and {How} to {Reduce} {Risk}},
	url = {https://openai.com/blog/forecasting-misuse/},
	abstract = {OpenAI researchers collaborated with Georgetown University’s Center for Security and Emerging Technology and the Stanford Internet Observatory to investigate how large language models might be misused for disinformation purposes. The collaboration included an October 2021 workshop bringing together 30 disinformation researchers, machine learning experts, and policy analysts, and culminated},
	language = {en},
	urldate = {2023-02-23},
	journal = {OpenAI},
	month = jan,
	year = {2023},
}

@misc{noauthor_how_2023,
	title = {How should {AI} systems behave, and who should decide?},
	url = {https://openai.com/blog/how-should-ai-systems-behave/},
	abstract = {We’re clarifying how ChatGPT's behavior is shaped and our plans for improving that behavior, allowing more user customization, and getting more public input into our decision-making in these areas.



OpenAI’s mission is to ensure that artificial general intelligence (AGI)[1] benefits all of humanity. We therefore think a},
	language = {en},
	urldate = {2023-02-23},
	journal = {OpenAI},
	month = feb,
	year = {2023},
}

@misc{suarez_neural_2019,
	title = {Neural {MMO}: {A} {Massively} {Multiagent} {Game} {Environment} for {Training} and {Evaluating} {Intelligent} {Agents}},
	shorttitle = {Neural {MMO}},
	url = {http://arxiv.org/abs/1903.00784},
	doi = {10.48550/arXiv.1903.00784},
	abstract = {The emergence of complex life on Earth is often attributed to the arms race that ensued from a huge number of organisms all competing for finite resources. We present an artificial intelligence research environment, inspired by the human game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games, a.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs and the real world alike, our environment is persistent and supports a large and variable number of agents. Our environment is well suited to the study of large-scale multiagent interaction: it requires that agents learn robust combat and navigation policies in the presence of large populations attempting to do the same. Baseline experiments reveal that population size magnifies and incentivizes the development of skillful behaviors and results in agents that outcompete agents trained in smaller populations. We further show that the policies of agents with unshared weights naturally diverge to fill different niches in order to avoid competition.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Suarez, Joseph and Du, Yilun and Isola, Phillip and Mordatch, Igor},
	month = mar,
	year = {2019},
	note = {arXiv:1903.00784 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
}

@misc{askell_role_2019,
	title = {The {Role} of {Cooperation} in {Responsible} {AI} {Development}},
	url = {http://arxiv.org/abs/1907.04534},
	doi = {10.48550/arXiv.1907.04534},
	abstract = {In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Askell, Amanda and Brundage, Miles and Hadfield, Gillian},
	month = jul,
	year = {2019},
	note = {arXiv:1907.04534 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, K.1, K.4.1},
}

@misc{tamkin_understanding_2021,
	title = {Understanding the {Capabilities}, {Limitations}, and {Societal} {Impact} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2102.02503},
	doi = {10.48550/arXiv.2102.02503},
	abstract = {On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
	month = feb,
	year = {2021},
	note = {arXiv:2102.02503 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{noauthor_international_nodate,
	title = {International {Handbook} of {Computer}-{Supported} {Collaborative} {Learning}},
	url = {https://link.springer.com/book/10.1007/978-3-030-65291-3},
	abstract = {This book will provide an overview of the diverse aspects of CSCL research, allowing readers to develop a sense of the entirety of CSCL research.},
	language = {en},
	urldate = {2023-02-22},
}
